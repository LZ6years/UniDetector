#!/usr/bin/env python3
"""
æ··åˆæ¨¡å¼è®­ç»ƒè„šæœ¬ - å®Œå…¨å…¼å®¹Jittorå’ŒMMDetection
å®ç°ä¸åŸå§‹PyTorchç‰ˆæœ¬å¯¹é½çš„ä¸¤é˜¶æ®µè®­ç»ƒ
"""

import argparse
import copy
import os
import os.path as osp
import time
import warnings
import sys
import numpy as np
import json
import pickle
import datetime
from tqdm import tqdm

import jittor as jt
import jittor.models as jm
import mmcv
from mmcv import Config, DictAction
from mmcv.runner import get_dist_info, init_dist
from mmcv.utils import get_git_hash

from mmdet import __version__
from mmdet.apis import set_random_seed
from mmdet.datasets import build_dataset

from mmdet.models.builder import build_head, build_roi_extractor, build_neck
from mmdet.utils import collect_env, get_root_logger


def parse_args():
    parser = argparse.ArgumentParser(description='æ··åˆæ¨¡å¼è®­ç»ƒæ£€æµ‹å™¨')
    parser.add_argument('config', help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--work-dir', help='ä¿å­˜æ—¥å¿—å’Œæ¨¡å‹çš„ç›®å½•')
    parser.add_argument('--resume-from', help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹æ–‡ä»¶')
    parser.add_argument('--no-validate', action='store_true', help='è®­ç»ƒæœŸé—´ä¸è¯„ä¼°æ£€æŸ¥ç‚¹')
    parser.add_argument('--epochs', type=int, default=4, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=2, help='æ‰¹æ¬¡å¤§å°')
    # ç§»é™¤æ‰‹åŠ¨stageå‚æ•°ï¼Œæ”¹ä¸ºè‡ªåŠ¨è¯†åˆ«
    group_gpus = parser.add_mutually_exclusive_group()
    group_gpus.add_argument('--gpus', type=int, help='ä½¿ç”¨çš„GPUæ•°é‡')
    group_gpus.add_argument('--gpu-ids', type=int, nargs='+', help='ä½¿ç”¨çš„GPU ID')
    parser.add_argument('--seed', type=int, default=None, help='éšæœºç§å­')
    parser.add_argument('--deterministic', action='store_true', help='æ˜¯å¦ä¸ºCUDNNåç«¯è®¾ç½®ç¡®å®šæ€§é€‰é¡¹')
    parser.add_argument('--options', nargs='+', action=DictAction, help='è¦†ç›–é…ç½®è®¾ç½®')
    parser.add_argument('--cfg-options', nargs='+', action=DictAction, help='è¦†ç›–é…ç½®è®¾ç½®')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm', 'mpi'], default='none', help='ä½œä¸šå¯åŠ¨å™¨')
    parser.add_argument('--local_rank', type=int, default=0)
    args = parser.parse_args()
    if 'LOCAL_RANK' not in os.environ:
        os.environ['LOCAL_RANK'] = str(args.local_rank)
    if args.options and args.cfg_options:
        raise ValueError('--options å’Œ --cfg-options ä¸èƒ½åŒæ—¶æŒ‡å®š')
    if args.options:
        warnings.warn('--options å·²è¢«å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ --cfg-options')
        args.cfg_options = args.options
    return args


def setup_jittor():
    """è®¾ç½®Jittorç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®Jittorç¯å¢ƒ...")
    
    # è®¾ç½®Jittorè°ƒè¯•ç¯å¢ƒå˜é‡
    os.environ['JT_SYNC'] = '1'  # å¼ºåˆ¶CUDAåŒæ­¥ï¼Œä¾¿äºè°ƒè¯•
    os.environ['trace_py_var'] = '3'  # å¯ç”¨Pythonå˜é‡è¿½è¸ª
    
    # è®¾ç½®cuDNNä¼˜åŒ–
    os.environ['CUDNN_CONV_ALGO_CACHE_MAX'] = '1000'  # å¢åŠ ç®—æ³•ç¼“å­˜å¤§å°
    os.environ['CUDNN_CONV_USE_DEFAULT_MATH'] = '1'   # ä½¿ç”¨é»˜è®¤æ•°å­¦æ¨¡å¼
    
    print(f"âœ… JT_SYNC: {os.environ.get('JT_SYNC', 'æœªè®¾ç½®')}")
    print(f"âœ… trace_py_var: {os.environ.get('trace_py_var', 'æœªè®¾ç½®')}")
    print(f"âœ… CUDNN_CONV_ALGO_CACHE_MAX: {os.environ.get('CUDNN_CONV_ALGO_CACHE_MAX', 'æœªè®¾ç½®')}")
    
    # è®¾ç½®Jittoræ ‡å¿—
    jt.flags.amp_level = 3  # è‡ªåŠ¨æ··åˆç²¾åº¦
    jt.flags.amp_reg = 1    # è‡ªåŠ¨æ··åˆç²¾åº¦æ³¨å†Œ
    
    print("âœ… Jittorç¯å¢ƒè®¾ç½®å®Œæˆ")


def load_custom_components():
    """åŠ è½½è‡ªå®šä¹‰ç»„ä»¶"""
    print("ğŸ“¦ åŠ è½½è‡ªå®šä¹‰ç»„ä»¶...")
    try:
        # æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥modelsæ¨¡å—
        current_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(current_dir)
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)
        
        # å¯¼å…¥æ‰€æœ‰Jittoræ¨¡å‹ç»„ä»¶ï¼Œç¡®ä¿å®ƒä»¬è¢«æ³¨å†Œåˆ°mmdetæ³¨å†Œè¡¨ä¸­
        from models.backbones.clip_backbone import CLIPResNet, CLIPResNetFPN
        from models.roi_heads.oln_roi_head import OlnRoIHead
        from models.roi_heads.bbox_heads.bbox_head_clip_partitioned import BBoxHeadCLIPPartitioned
        from models.roi_heads.bbox_heads.shared2fc_bbox_score_head import Shared2FCBBoxScoreHead
        from models.heads.rpn_head import RPNHead
        from models.heads.oln_rpn_head import OlnRPNHead
        from models.necks.fpn import FPN
        from models.detectors.faster_rcnn import FasterRCNN
        from models.detectors.fast_rcnn import FastRCNN
        from models.roi_heads.roi_extractors.single_roi_extractor import SingleRoIExtractor
        
        print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰Jittoræ¨¡å‹ç»„ä»¶")
        return True
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å‹ç»„ä»¶å¤±è´¥: {e}")
        return False

def detect_training_stage(cfg, config_path):
    """è‡ªåŠ¨æ£€æµ‹è®­ç»ƒé˜¶æ®µ"""
    print("ğŸ” è‡ªåŠ¨æ£€æµ‹è®­ç»ƒé˜¶æ®µ...")
    
    # æ–¹æ³•1ï¼šä»é…ç½®æ–‡ä»¶åæ£€æµ‹
    config_name = os.path.basename(config_path).lower()
    if '1st' in config_name or 'rpn' in config_name:
        stage = '1st'
        print(f"ğŸ“‹ ä»é…ç½®æ–‡ä»¶åæ£€æµ‹åˆ°ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ: {config_name}")
    elif '2nd' in config_name or 'roi' in config_name or 'rcnn' in config_name:
        stage = '2nd'
        print(f"ğŸ“‹ ä»é…ç½®æ–‡ä»¶åæ£€æµ‹åˆ°ç¬¬äºŒé˜¶æ®µè®­ç»ƒ: {config_name}")
    else:
        # æ–¹æ³•2ï¼šä»æ¨¡å‹é…ç½®æ£€æµ‹
        model_type = cfg.model.type
        if model_type == 'FasterRCNN':
            stage = '1st'
            print(f"ğŸ“‹ ä»æ¨¡å‹ç±»å‹æ£€æµ‹åˆ°ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ: {model_type}")
        elif model_type == 'FastRCNN':
            stage = '2nd'
            print(f"ğŸ“‹ ä»æ¨¡å‹ç±»å‹æ£€æµ‹åˆ°ç¬¬äºŒé˜¶æ®µè®­ç»ƒ: {model_type}")
        else:
            # æ–¹æ³•3ï¼šä»backboneç±»å‹æ£€æµ‹
            backbone_type = cfg.model.backbone.type
            if 'CLIP' in backbone_type:
                stage = '2nd'
                print(f"ğŸ“‹ ä»backboneç±»å‹æ£€æµ‹åˆ°ç¬¬äºŒé˜¶æ®µè®­ç»ƒ: {backbone_type}")
            else:
                stage = '1st'
                print(f"ğŸ“‹ é»˜è®¤ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ: {backbone_type}")
    
    print(f"âœ… æ£€æµ‹åˆ°è®­ç»ƒé˜¶æ®µ: {stage}")
    return stage


def safe_convert_to_jittor(data, max_depth=10, current_depth=0):
    """å®‰å…¨åœ°å°†æ•°æ®è½¬æ¢ä¸ºJittoræ ¼å¼ï¼Œé¿å…æ— é™é€’å½’"""
    if current_depth >= max_depth:
        return data  # é˜²æ­¢æ— é™é€’å½’

    # å¯¹åŸå§‹/ç®€å•ç±»å‹ç›´æ¥è¿”å›ï¼Œé¿å…ä¸å¿…è¦åˆ¤å®š
    try:
        if data is None or isinstance(data, (int, float, bool, str, bytes)):
            return data
    except RecursionError:
        return data

    try:
        # å¦‚æœå·²ç»æ˜¯Jittorå˜é‡ï¼Œç›´æ¥è¿”å›
        is_jt_var = False
        try:
            is_jt_var = isinstance(data, jt.Var)
        except RecursionError:
            # å¦‚æœåœ¨ __instancecheck__ ä¸­é€’å½’ï¼Œç›´æ¥è·³è¿‡è½¬æ¢
            return data
        if is_jt_var:
            return data

        # å¦‚æœæ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œé€’å½’å¤„ç†
        if isinstance(data, (list, tuple)):
            return [safe_convert_to_jittor(item, max_depth, current_depth + 1) for item in data]

        # å¦‚æœæ˜¯å­—å…¸ï¼Œé€’å½’å¤„ç†
        if isinstance(data, dict):
            return {key: safe_convert_to_jittor(value, max_depth, current_depth + 1) for key, value in data.items()}

        # å¤„ç†ç±»ä¼¼ DataContainer çš„å¯¹è±¡ï¼ˆæœ‰ data å±æ€§ï¼‰
        if hasattr(data, 'data') and not is_jt_var:
            # æ£€æŸ¥æ˜¯å¦å¯å †å 
            if hasattr(data, 'stack') and getattr(data, 'stack', False):
                return safe_convert_to_jittor(getattr(data, 'data'), max_depth, current_depth + 1)
            # é stack æƒ…å†µï¼šåˆ—è¡¨
            inner = getattr(data, 'data')
            if isinstance(inner, list):
                converted_list = []
                for item in inner:
                    if hasattr(item, 'cpu') and hasattr(item, 'numpy'):
                        try:
                            converted_list.append(jt.array(item.cpu().numpy()))
                        except Exception:
                            converted_list.append(item)
                    elif hasattr(item, 'shape') and hasattr(item, 'dtype'):
                        try:
                            converted_list.append(jt.array(item))
                        except Exception:
                            converted_list.append(item)
                    else:
                        converted_list.append(item)
                return converted_list
            return safe_convert_to_jittor(inner, max_depth, current_depth + 1)

        # å¦‚æœæ˜¯PyTorchå¼ é‡ï¼Œè½¬æ¢ä¸ºJittor
        if hasattr(data, 'cpu') and hasattr(data, 'numpy') and not is_jt_var:
            try:
                return jt.array(data.cpu().numpy())
            except Exception:
                return data

        # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºJittor
        if hasattr(data, 'shape') and hasattr(data, 'dtype') and not is_jt_var:
            try:
                return jt.array(data)
            except Exception:
                return data

        # å¦‚æœæ˜¯memoryviewï¼Œè½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(data, memoryview):
            try:
                import numpy as np
                return jt.array(np.array(data))
            except Exception:
                return data

        # å…¶ä»–æƒ…å†µï¼Œç›´æ¥è¿”å›
        return data

    except Exception:
        # é™é»˜å¤„ç†é”™è¯¯ï¼Œé¿å…å¤§é‡é”™è¯¯è¾“å‡º
        return data


def create_jittor_compatible_model(cfg, stage='1st'):
    """åˆ›å»ºJittorå…¼å®¹çš„æ¨¡å‹ - ä½¿ç”¨å·²æœ‰ç»„ä»¶"""
    print(f"ğŸ”§ åˆ›å»ºJittorå…¼å®¹æ¨¡å‹ - é˜¶æ®µ: {stage}")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯
    print(f"ğŸ“‹ æ¨¡å‹é…ç½®ä¿¡æ¯:")
    print(f"   - æ¨¡å‹ç±»å‹: {cfg.model.type}")
    print(f"   - Backboneç±»å‹: {cfg.model.backbone.type}")
    if hasattr(cfg.model, 'neck'):
        print(f"   - Neckç±»å‹: {cfg.model.neck.type}")
    if hasattr(cfg.model, 'rpn_head'):
        print(f"   - RPN Headç±»å‹: {cfg.model.rpn_head.type}")
    if hasattr(cfg.model, 'roi_head'):
        print(f"   - ROI Headç±»å‹: {cfg.model.roi_head.type}")
    
    # å¯¼å…¥å·²æœ‰çš„æ¨¡å‹ç»„ä»¶
    try:
        # å¯¼å…¥Jittorç‰ˆæœ¬çš„æ¨¡å‹ç»„ä»¶
        from models.backbones.clip_backbone import CLIPResNet
        from models.roi_heads.oln_roi_head import OlnRoIHead
        from models.roi_heads.bbox_heads.bbox_head_clip_partitioned import BBoxHeadCLIPPartitioned
        from models.roi_heads.bbox_heads.shared2fc_bbox_score_head import Shared2FCBBoxScoreHead
        from models.heads.rpn_head import RPNHead
        print("âœ… æˆåŠŸå¯¼å…¥å·²æœ‰æ¨¡å‹ç»„ä»¶")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å‹ç»„ä»¶å¤±è´¥: {e}")
        raise
    
    # åˆ›å»ºåŸºäºå·²æœ‰ç»„ä»¶çš„Jittoræ¨¡å‹
    class JittorModelWithComponents(jt.Module):
        def __init__(self, cfg, stage='1st'):
            super().__init__()
            self.cfg = cfg
            self.stage = stage
            
            # ä»é…ç½®æ–‡ä»¶ä¸­æå–å‚æ•°
            model_cfg = cfg.model
            
            if stage == '1st':
                # ç¬¬ä¸€é˜¶æ®µï¼šFasterRCNN
                self._build_1st_stage_with_components(model_cfg)

            
            print(f"âœ… Jittoræ¨¡å‹åˆ›å»ºæˆåŠŸ - é˜¶æ®µ: {stage}")
        
        def execute(self, img, gt_bboxes=None, gt_labels=None, **kwargs):
            """Jittoræ¨¡å‹çš„executeæ–¹æ³•ï¼Œå¤„ç†å‰å‘ä¼ æ’­"""
            if self.stage == '1st':
                # è·å–batch size
                if hasattr(img, 'shape'):
                    batch_size = img.shape[0]
                elif isinstance(img, (list, tuple)) and len(img) > 0:
                    if hasattr(img[0], 'shape'):
                        batch_size = img[0].shape[0]
                    else:
                        batch_size = 1
                else:
                    batch_size = 1
                
                return self._forward_1st_stage_with_components(img, gt_bboxes, gt_labels, batch_size)
            else:
                raise NotImplementedError(f"Stage {self.stage} not implemented")
        
        def _build_1st_stage_with_components(self, model_cfg):
            """ä½¿ç”¨å·²æœ‰ç»„ä»¶æ„å»ºç¬¬ä¸€é˜¶æ®µæ¨¡å‹"""
            print("ğŸ”§ ä½¿ç”¨å·²æœ‰ç»„ä»¶æ„å»ºç¬¬ä¸€é˜¶æ®µæ¨¡å‹...")
            
            # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–backboneå‚æ•°
            backbone_cfg = model_cfg.backbone
            depth = backbone_cfg.get('depth', 50)
            print(f"   - Backbone: ResNet{depth}")

            # ä½¿ç”¨ Jittor æä¾›çš„ ResNet50 + ImageNet é¢„è®­ç»ƒæƒé‡
            self.resnet = jm.resnet50(pretrained=True)
            print("å·²åŠ è½½ jittor.models.resnet50(pretrained=True)")

            # æ ¹æ®é…ç½®å†»ç»“å‰è‹¥å¹² stageï¼ˆconv1+bn1 è®°ä¸º stage 0ï¼Œlayer1 ä¸º stage 1ï¼‰
            frozen_stages = getattr(backbone_cfg, 'frozen_stages', 0)
            if isinstance(frozen_stages, int) and frozen_stages >= 0:
                def _stop_grad_module(mod):
                    for p in mod.parameters():
                        try:
                            p.stop_grad()
                        except Exception:
                            pass
                if frozen_stages >= 0:
                    for name in ['conv1', 'bn1']:
                        if hasattr(self.resnet, name):
                            _stop_grad_module(getattr(self.resnet, name))
                if frozen_stages >= 1 and hasattr(self.resnet, 'layer1'):
                    _stop_grad_module(self.resnet.layer1)
                if frozen_stages >= 2 and hasattr(self.resnet, 'layer2'):
                    _stop_grad_module(self.resnet.layer2)
                if frozen_stages >= 3 and hasattr(self.resnet, 'layer3'):
                    _stop_grad_module(self.resnet.layer3)
                if frozen_stages >= 4 and hasattr(self.resnet, 'layer4'):
                    _stop_grad_module(self.resnet.layer4)
                print(f"   ğŸ”’ å·²å†»ç»“å‰ {frozen_stages} ä¸ª stage")

            # norm_eval: å°† BN è®¾ä¸º eval æ¨¡å¼ï¼Œä¿æŒå‡å€¼æ–¹å·®ä¸æ›´æ–°
            norm_eval = bool(getattr(backbone_cfg, 'norm_eval', False))
            if norm_eval:
                try:
                    for m in self.resnet.modules():
                        # å…¼å®¹ä¸åŒ BN ç±»å
                        if m.__class__.__name__ in ("BatchNorm", "BatchNorm2d"):
                            try:
                                m.is_training = False
                            except Exception:
                                pass
                except Exception:
                    pass
                print("   ğŸ§Š å·²å°† BatchNorm ç½®ä¸º eval (norm_eval=True)")

            # ä¸ä½¿ç”¨åˆ†ç±» fcï¼Œç¦ç”¨å…¶æ¢¯åº¦ä»¥é¿å…æ— æ¢¯åº¦è­¦å‘Š
            try:
                if hasattr(self.resnet, 'fc'):
                    for p in self.resnet.fc.parameters():
                        p.stop_grad()
            except Exception:
                pass

            # ä½¿ç”¨FPN neck
            if hasattr(model_cfg, 'neck'):
                neck_cfg = model_cfg.neck
                print(f"   - Neck: {neck_cfg.type}, out_channels: {neck_cfg.out_channels}")
                self.fpn_out_channels = neck_cfg.out_channels
                self.neck = build_neck(neck_cfg)
                print("   âœ… å·²æ„å»º FPN")

            # ä½¿ç”¨RPN head
            if hasattr(model_cfg, 'rpn_head'):
                rpn_cfg = model_cfg.rpn_head
                # ä¼˜å…ˆå°è¯• OLN-RPNHead
                from models.heads.oln_rpn_head import OlnRPNHead as JT_RPNHead
                print("   âœ… ä½¿ç”¨ Jittor OlnRPNHead")
                # ä»¥ FPN è¾“å‡ºé€šé“ä½œä¸ºè¾“å…¥é€šé“ï¼Œfeat_channels æ¥è‡ªé…ç½®
                self.rpn_head_jt = JT_RPNHead(
                    in_channels=self.fpn_out_channels,
                    feat_channels=rpn_cfg.feat_channels,
                    anchor_generator=getattr(rpn_cfg, 'anchor_generator', None),
                )
            # è®°å½• FPN strides ä¾› proposals ç”Ÿæˆä½¿ç”¨
            self.fpn_strides = None
            if hasattr(rpn_cfg, 'anchor_generator') and hasattr(rpn_cfg.anchor_generator, 'strides'):
                self.fpn_strides = list(rpn_cfg.anchor_generator.strides)

            # ä½¿ç”¨ROI head
            if hasattr(model_cfg, 'roi_head'):
                roi_cfg = model_cfg.roi_head
                # print(f"   - ROI: {roi_cfg.type}")
                if hasattr(roi_cfg, 'bbox_head'):
                    bbox_cfg = roi_cfg.bbox_head
                    # print(f"   - BBox Head: {bbox_cfg.type}")
                    # æ„å»ºæ¨¡å—åŒ–çš„ RoIExtractor ä¸ BBoxHead
                    if hasattr(roi_cfg, 'bbox_roi_extractor'):
                        self.roi_extractor = build_roi_extractor(roi_cfg.bbox_roi_extractor)
                        print("   âœ… å·²æ„å»º SingleRoIExtractor")

                        self.bbox_head = build_head(bbox_cfg)
                        self.roi_feat_size = bbox_cfg.get('roi_feat_size', 7)
                        print("   âœ… å·²æ„å»º BBoxHead æ¨¡å—")

        
                
        def _forward_1st_stage_with_components(self, img, gt_bboxes, gt_labels, batch_size):
            """ä½¿ç”¨å·²æœ‰ç»„ä»¶çš„ç¬¬ä¸€é˜¶æ®µå‰å‘ä¼ æ’­"""
            # å¤„ç†imgå‚æ•°ï¼šå¦‚æœæ˜¯åˆ—è¡¨ï¼Œæå–ç¬¬ä¸€ä¸ªå…ƒç´ 
            if isinstance(img, (list, tuple)) and len(img) > 0:
                img = img[0]  # æå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                print(f"ğŸ” ä»åˆ—è¡¨ä¸­æå–imgï¼Œshape: {img.shape}")
            
            # å¼ºåŒ–å›¾åƒå¼ é‡å¥å£®æ€§ï¼šç¡®ä¿ä¸º jt.Varã€float32ã€NCHW
            try:
                if not isinstance(img, jt.Var):
                    import numpy as _np
                    img = jt.array(_np.array(img))
            except Exception:
                pass
            if hasattr(img, 'shape') and len(img.shape) == 3:
                img = img.unsqueeze(0)
            try:
                img = img.float32()
            except Exception:
                pass
            # æå–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„gt_bboxå’Œgt_labelç”¨äºæŸå¤±è®¡ç®—
            if len(gt_bboxes) > 0 and hasattr(gt_bboxes[0], 'view'):
                gt_bbox = gt_bboxes[0]
            else:
                gt_bbox = jt.randn(1, 4) * 0.01
                
            if len(gt_labels) > 0 and hasattr(gt_labels[0], 'view'):
                gt_label = gt_labels[0]
            else:
                gt_label = jt.randn(1) * 0.01
            
            # Backboneç‰¹å¾æå–ï¼ˆä¼˜å…ˆä½¿ç”¨ jittor resnet50ï¼Œå¦åˆ™å›é€€åˆ°ç®€åŒ–ç‰ˆï¼‰
            if hasattr(self, 'resnet') and self.resnet is not None:
                x = img
                x = self.resnet.conv1(x)
                x = self.resnet.bn1(x)
                x = self.resnet.relu(x)
                x = self.resnet.maxpool(x)
                c2 = self.resnet.layer1(x)
                c3 = self.resnet.layer2(c2)
                c4 = self.resnet.layer3(c3)
                c5 = self.resnet.layer4(c4)
                feat = c5  # [B, 2048, H/32, W/32]

            
            # FPN ç‰¹å¾èåˆ
            if hasattr(self, 'neck') and self.neck is not None:
                fpn_feats = self.neck([c2, c3, c4, c5])
                fpn_rpn = fpn_feats[-1]
                num_roi_lvls = 4
                if getattr(self, 'roi_extractor', None) is not None and hasattr(self.roi_extractor, 'featmap_strides'):
                    try:
                        num_roi_lvls = len(self.roi_extractor.featmap_strides)
                    except Exception:
                        num_roi_lvls = 4
                feats_pyramid = list(fpn_feats[:num_roi_lvls])
            else:
                fpn_rpn = self.fpn(feat)
                feats_pyramid = [fpn_rpn]
            
            # RPNå‰å‘ä¼ æ’­ï¼ˆä¼˜å…ˆä½¿ç”¨ Jittor RPNHeadï¼‰
            if hasattr(self, 'rpn_head_jt') and self.rpn_head_jt is not None:
                # ä¼ å…¥å…¨éƒ¨ FPN å±‚ï¼ŒRPNHead å†…éƒ¨æ”¯æŒå¤šå±‚
                rpn_out = self.rpn_head_jt(fpn_feats if 'fpn_feats' in locals() else fpn_rpn)
                if isinstance(rpn_out, (list, tuple)) and len(rpn_out) == 3:
                    rpn_cls, rpn_reg, rpn_obj = rpn_out
                else:
                    rpn_cls, rpn_reg = rpn_out
                    rpn_obj = None

            
            # ä½¿ç”¨æ¨¡å—åŒ– RoIExtractor + BBoxHead
            if getattr(self, 'roi_extractor', None) is not None and getattr(self, 'bbox_head', None) is not None:
                # ç”Ÿæˆ proposalsï¼šè‹¥ä¸º OLN-RPNHeadï¼Œä½¿ç”¨å…¶å†…ç½® get_bboxes
                if rpn_obj is not None and hasattr(self.rpn_head_jt, 'get_bboxes'):
                    try:
                        rois = self.rpn_head_jt.get_bboxes(
                            rpn_cls, rpn_reg, rpn_obj,
                            img_shape=img.shape,
                            cfg=(self.cfg.test_cfg.rpn if hasattr(self.cfg, 'test_cfg') and hasattr(self.cfg.test_cfg, 'rpn') else None)
                        )
                    except Exception as e:
                        print(f"âš ï¸  RPN get_bboxes å¤±è´¥: {e}")
                        rois = None
                else:
                    # æ—§çš„ç®€åŒ–ä¸­å¿ƒæ¡† proposals
                    rois = self._generate_simple_proposals(
                        rpn_cls, fpn_feats if 'fpn_feats' in locals() else [fpn_rpn],
                        strides=(self.fpn_strides if self.fpn_strides is not None else [4,8,16,32,64]),
                        nms_pre=getattr(self.cfg.test_cfg.rpn, 'nms_pre', 1000) if hasattr(self.cfg, 'test_cfg') else 1000,
                        max_num=getattr(self.cfg.test_cfg.rpn, 'max_num', 1000) if hasattr(self.cfg, 'test_cfg') else 1000,
                        nms_thr=getattr(self.cfg.test_cfg.rpn, 'nms_thr', 0.7) if hasattr(self.cfg, 'test_cfg') else 0.7,
                        img_shape=img.shape
                    )

                # ä¸ºèŠ‚çœæ˜¾å­˜ï¼Œè®­ç»ƒé˜¶æ®µæŒ‰å›¾é‡‡æ ·æœ€å¤š K ä¸ª RoIs è¿›å…¥ ROI Head
                try:
                    max_rois_per_img_train = 2000
                    if hasattr(self.cfg, 'train_cfg') and hasattr(self.cfg.train_cfg, 'rcnn'):
                        # è‹¥åç»­æ¥å…¥ assigner/samplerï¼Œå¯ä»æ­¤å¤„è¯»å–ç›®æ ‡é‡‡æ ·é‡
                        pass
                    if rois is not None and isinstance(rois, jt.Var) and rois.shape[0] > 0:
                        bidx = rois[:, 0]
                        keep_indices = []
                        for b in range(int(img.shape[0])):
                            nz = (bidx == float(b)).nonzero()
                            if nz.shape[0] == 0:
                                continue
                            idxs = nz.reshape(-1)
                            take = min(max_rois_per_img_train, int(idxs.shape[0]))
                            keep_indices.append(idxs[:take])
                        if len(keep_indices) > 0:
                            keep_indices = jt.concat(keep_indices, dim=0)
                            rois = rois[keep_indices, :]
                except Exception:
                    pass
                    
                # é˜²å¾¡ï¼šç¡®ä¿è¾“å…¥ä¸º Jittor Var
                feats_pyramid = [f if isinstance(f, jt.Var) else jt.array(f) for f in feats_pyramid]
                rois = rois if isinstance(rois, jt.Var) else jt.array(rois)
                try:
                    self._last_num_rois = int(rois.shape[0])
                except Exception:
                    self._last_num_rois = None
                roi_feat = self.roi_extractor(feats_pyramid, rois)
                bh_out = self.bbox_head(roi_feat)
                if isinstance(bh_out, (list, tuple)):
                    if len(bh_out) == 3:
                        roi_cls, roi_reg, roi_score = bh_out
                    elif len(bh_out) == 2:
                        roi_cls, roi_reg = bh_out
                        roi_score = None
                    else:
                        roi_cls = bh_out[0]
                        roi_reg = bh_out[1] if len(bh_out) > 1 else None
                        roi_score = None
                else:
                    roi_cls = bh_out
                    roi_reg = None
                    roi_score = None

                # æ„å»º ROI è®­ç»ƒç›®æ ‡ï¼ˆç®€åŒ–çœŸå®ç‰ˆï¼‰
                labels, label_weights, bbox_targets, bbox_weights, bbox_score_targets, bbox_score_weights = \
                    self.bbox_head.build_targets_minimal(
                        rois,
                        gt_bboxes,
                        img_shape=img.shape,
                        pos_iou_thr=getattr(self.cfg.train_cfg.rcnn.assigner, 'pos_iou_thr', 0.5) if hasattr(self.cfg, 'train_cfg') else 0.5,
                        neg_iou_thr=getattr(self.cfg.train_cfg.rcnn.assigner, 'neg_iou_thr', 0.5) if hasattr(self.cfg, 'train_cfg') else 0.5,
                        num_samples=getattr(self.cfg.train_cfg.rcnn.sampler, 'num', 256) if hasattr(self.cfg, 'train_cfg') else 256,
                        pos_fraction=getattr(self.cfg.train_cfg.rcnn.sampler, 'pos_fraction', 0.25) if hasattr(self.cfg, 'train_cfg') else 0.25,
                    )

                # è®¡ç®— ROI æŸå¤±ï¼ˆä¸€æ¬¡æ€§ä¼ å…¥ bbox_score åŠå…¶ç›®æ ‡ï¼‰
                roi_losses = {}
                if hasattr(self.bbox_head, 'loss'):
                    num_classes = getattr(self.bbox_head, 'num_classes', 1)
                    roi_losses = self.bbox_head.loss(
                        roi_cls if roi_cls is not None else jt.zeros((labels.shape[0], num_classes+1)),
                        roi_reg if roi_reg is not None else jt.zeros((labels.shape[0], 4)),
                        roi_score if roi_score is not None else jt.zeros((labels.shape[0], 1)),
                        rois,
                        labels,
                        label_weights,
                        bbox_targets,
                        bbox_weights,
                        bbox_score_targets=bbox_score_targets,
                        bbox_score_weights=bbox_score_weights,
                    )
            
            # è®¡ç®—æŸå¤± (çœŸå® RPN è®­ç»ƒ)
            if hasattr(self, 'rpn_head_jt') and self.rpn_head_jt is not None and rpn_obj is not None and hasattr(self.rpn_head_jt, 'loss'):
                try:
                    # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°è°ƒç”¨RPN losså‡½æ•°
                    rpn_losses = self.rpn_head_jt.loss(
                        [rpn_cls], [rpn_reg], [rpn_obj],
                        gt_bboxes_list=gt_bboxes,
                        img_shape=img.shape  # ä¼ é€’å®Œæ•´çš„img_shape (B, C, H, W)
                    )
                    rpn_cls_loss = rpn_losses.get('loss_rpn_cls', jt.zeros(1))
                    rpn_bbox_loss = rpn_losses.get('loss_rpn_bbox', jt.zeros(1))
                    rpn_obj_loss = rpn_losses.get('loss_rpn_obj', jt.zeros(1))
                except Exception as e:
                    print(f"âš ï¸  RPNæŸå¤±è®¡ç®—å¤±è´¥: {e}")
                    # å›é€€åˆ°ç®€åŒ–æŸå¤±ï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                    if hasattr(rpn_cls, 'shape'):
                        rpn_cls_loss = jt.mean(jt.sqr(rpn_cls)) * 0.0  # æŒ‰é…ç½®æƒé‡ä¸º0
                    else:
                        rpn_cls_loss = jt.zeros(1) * 0.0
                    
                    if hasattr(rpn_reg, 'shape'):
                        rpn_bbox_loss = jt.mean(jt.sqr(rpn_reg)) * 10.0  # æŒ‰é…ç½®æƒé‡ä¸º10
                    else:
                        rpn_bbox_loss = jt.zeros(1) * 10.0
                    
                    if hasattr(rpn_obj, 'shape'):
                        rpn_obj_loss = jt.mean(jt.sqr(rpn_obj)) * 1.0   # æŒ‰é…ç½®æƒé‡ä¸º1
                    else:
                        rpn_obj_loss = jt.zeros(1) * 1.0
            else:
                # æŒ‰é…ç½®æ–‡ä»¶æƒé‡è®¡ç®—æŸå¤±ï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                if hasattr(rpn_cls, 'shape'):
                    rpn_cls_loss = jt.mean(jt.sqr(rpn_cls)) * 0.0  # loss_weight=0.0
                else:
                    rpn_cls_loss = jt.zeros(1) * 0.0
                
                if hasattr(rpn_reg, 'shape'):
                    rpn_bbox_loss = jt.mean(jt.sqr(rpn_reg)) * 10.0  # loss_weight=10.0
                else:
                    rpn_bbox_loss = jt.zeros(1) * 10.0
                
                if hasattr(rpn_obj, 'shape'):
                    rpn_obj_loss = jt.mean(jt.sqr(rpn_obj)) * 1.0   # loss_weight=1.0
                else:
                    rpn_obj_loss = jt.zeros(1) * 1.0
            
            # ROI æŸå¤±è®¡ç®—
            if 'roi_losses' in locals() and isinstance(roi_losses, dict) and len(roi_losses) > 0:
                # ä½¿ç”¨çœŸå®çš„ROIæŸå¤±
                rcnn_cls_loss = roi_losses.get('loss_cls', jt.zeros(1)) * 1.0  # loss_weight=1.0
                rcnn_bbox_loss = roi_losses.get('loss_bbox', jt.zeros(1)) * 1.0  # loss_weight=1.0
                rcnn_score_loss = roi_losses.get('loss_bbox_score', jt.zeros(1)) * 1.0  # loss_weight=1.0
            else:
                # å ä½æŸå¤±ï¼Œä½¿ç”¨é…ç½®æƒé‡ï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                if hasattr(roi_cls, 'shape'):
                    rcnn_cls_loss = jt.mean(jt.sqr(roi_cls)) * 1.0  # loss_weight=1.0
                else:
                    rcnn_cls_loss = jt.zeros(1) * 1.0
                
                if hasattr(roi_reg, 'shape'):
                    rcnn_bbox_loss = jt.mean(jt.sqr(roi_reg)) * 1.0  # loss_weight=1.0
                else:
                    rcnn_bbox_loss = jt.zeros(1) * 1.0
                
                if hasattr(roi_score, 'shape'):
                    rcnn_score_loss = jt.mean(jt.sqr(roi_score)) * 1.0  # loss_weight=1.0
                else:
                    rcnn_score_loss = jt.zeros(1) * 1.0
            
            # æ±‡æ€»æ€»æŸå¤±
            total_loss = rpn_cls_loss + rpn_bbox_loss + rpn_obj_loss + rcnn_cls_loss + rcnn_bbox_loss + rcnn_score_loss
            
            return {
                'loss': total_loss,
                'rpn_cls_loss': rpn_cls_loss,
                'rpn_bbox_loss': rpn_bbox_loss,
                'rpn_obj_loss': rpn_obj_loss,
                'rcnn_cls_loss': rcnn_cls_loss,
                'rcnn_bbox_loss': rcnn_bbox_loss,
                'rcnn_score_loss': rcnn_score_loss
            }

        def _roi_align_first_gt(self, feat, gt_bboxes_list, output_size=7, stride=32):
            """åœ¨å•å±‚ç‰¹å¾å›¾ä¸Šï¼Œç”¨æ¯å¼ å›¾çš„é¦–ä¸ª GT æ¡†åšç®€æ˜“ RoIAlignã€‚
            - feat: [B, C, H, W]
            - gt_bboxes_list: List[Var[N, 4]] in image coords
            è¿”å›: [B, C, output_size, output_size]
            """
            B, C, H, W = feat.shape
            pooled = []
            for n in range(B):
                # å–ç¬¬ n å¼ å›¾çš„é¦–ä¸ª gt æ¡†
                box = None
                if isinstance(gt_bboxes_list, (list, tuple)) and len(gt_bboxes_list) > n:
                    b = gt_bboxes_list[n]
                    if hasattr(b, 'shape') and b.shape[0] > 0:
                        box = b[0]
                if box is None:
                    # è‹¥æ—  gtï¼Œé€€åŒ–ä¸ºæ•´å›¾æ± åŒ–
                    crop = feat[n:n+1, :, :, :]
                else:
                    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]
                    # è½¬åˆ°ç‰¹å¾åæ ‡å¹¶è£å‰ªåˆ°è¾¹ç•Œ
                    xs1 = jt.maximum((x1 / stride).floor().int32(), jt.int32(0))
                    ys1 = jt.maximum((y1 / stride).floor().int32(), jt.int32(0))
                    xs2 = jt.minimum((x2 / stride).ceil().int32(), jt.int32(W-1))
                    ys2 = jt.minimum((y2 / stride).ceil().int32(), jt.int32(H-1))
                    # é˜²æ­¢ç©ºåŒºåŸŸ
                    xs2 = jt.maximum(xs2, xs1 + 1)
                    ys2 = jt.maximum(ys2, ys1 + 1)
                    crop = feat[n:n+1, :, ys1:ys2, xs1:xs2]
                pooled.append(jt.nn.AdaptiveAvgPool2d((output_size, output_size))(crop))
            return jt.concat(pooled, dim=0)

        def _nms_numpy(self, boxes_np, scores_np, iou_thr=0.7, max_num=1000):
            # boxes: [N,4] (x1,y1,x2,y2) in numpy
            x1 = boxes_np[:, 0]
            y1 = boxes_np[:, 1]
            x2 = boxes_np[:, 2]
            y2 = boxes_np[:, 3]
            areas = (x2 - x1 + 1) * (y2 - y1 + 1)
            order = scores_np.argsort()[::-1]
            keep = []
            while order.size > 0 and len(keep) < max_num:
                i = order[0]
                keep.append(i)
                xx1 = np.maximum(x1[i], x1[order[1:]])
                yy1 = np.maximum(y1[i], y1[order[1:]])
                xx2 = np.minimum(x2[i], x2[order[1:]])
                yy2 = np.minimum(y2[i], y2[order[1:]])
                w = np.maximum(0.0, xx2 - xx1 + 1)
                h = np.maximum(0.0, yy2 - yy1 + 1)
                inter = w * h
                iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
                inds = np.where(iou <= iou_thr)[0]
                order = order[inds + 1]
            return keep

        def _generate_simple_proposals(self, rpn_cls, fpn_feats, strides, nms_pre=1000, max_num=1000, nms_thr=0.7, img_shape=None):
            # è¿”å› rois Var [N,5] (b,x1,y1,x2,y2) åˆå¹¶æ‰€æœ‰ batch
            import numpy as _np
            if not isinstance(fpn_feats, (list, tuple)):
                fpn_feats = [fpn_feats]
            if isinstance(rpn_cls, (list, tuple)):
                cls_list = rpn_cls
            else:
                cls_list = [rpn_cls]

            B = int(img_shape[0]) if img_shape is not None else 1
            rois_concat = []
            
            try:
                for b in range(B):
                    boxes_all = []
                    scores_all = []
                    for lvl, cls_map in enumerate(cls_list):
                        stride = strides[lvl] if lvl < len(strides) else strides[-1]
                        # cls_map: [B, A*2, H, W] -> å‰æ™¯åˆ†æ•° [H*W*A]
                        H, W = int(cls_map.shape[2]), int(cls_map.shape[3])
                        A2 = int(cls_map.shape[1])
                        A = max(1, A2 // 2)
                        
                        # å®‰å…¨åœ°è·å–æ‰¹æ¬¡æ•°æ®
                        if b < cls_map.shape[0]:
                            logits = cls_map[b:b+1, :, :, :]
                        else:
                            continue
                            
                        # ç¡®ä¿å¼ é‡å½¢çŠ¶æ­£ç¡®
                        if logits.shape[1] != A2:
                            continue
                            
                        logits = logits.reshape(1, A, 2, H, W)
                        probs = jt.softmax(logits, dim=2)  # softmax over class dim
                        fg = probs[:, :, 1, :, :].reshape(-1)
                        
                        # ç”Ÿæˆä¸­å¿ƒç‚¹ boxesï¼ˆç®€åŒ–ï¼‰ï¼šä»¥ stride ä¸ºè¾¹é•¿çš„æ­£æ–¹å½¢ï¼Œä¹˜ä»¥æ¯”ä¾‹ 8
                        scale = 8.0
                        # ç½‘æ ¼ä¸­å¿ƒ - ä½¿ç”¨numpyé¿å…Jittorå¼ é‡æ“ä½œé—®é¢˜
                        yy, xx = _np.meshgrid(_np.arange(H), _np.arange(W), indexing='ij')
                        xx = (xx + 0.5) * stride
                        yy = (yy + 0.5) * stride
                        
                        # è½¬æ¢ä¸ºJittorå¼ é‡
                        xx = jt.array(xx.astype(_np.float32))
                        yy = jt.array(yy.astype(_np.float32))
                        
                        # æ‰©å±•ç»´åº¦
                        xx = xx.unsqueeze(0).unsqueeze(0).expand(A, 1, H, W)
                        yy = yy.unsqueeze(0).unsqueeze(0).expand(A, 1, H, W)
                        
                        half = (stride * scale) / 2.0
                        x1 = (xx - half).clamp(0, float(img_shape[3]-1))
                        y1 = (yy - half).clamp(0, float(img_shape[2]-1))
                        x2 = (xx + half).clamp(0, float(img_shape[3]-1))
                        y2 = (yy + half).clamp(0, float(img_shape[2]-1))
                        
                        # é‡å¡‘å¹¶è½¬ç½®
                        boxes = jt.stack([x1, y1, x2, y2], dim=1).reshape(4, -1).transpose(1, 0)
                        
                        # é€‰ top-k
                        k = int(min(nms_pre, boxes.shape[0]))
                        if k <= 0:
                            continue
                            
                        # å®‰å…¨åœ°è½¬æ¢ä¸ºnumpy
                        try:
                            scores_np = fg.numpy()
                            boxes_np = boxes.numpy()
                        except Exception as e:
                            print(f"âš ï¸  å¼ é‡è½¬æ¢å¤±è´¥: {e}")
                            continue
                            
                        if len(scores_np) == 0 or len(boxes_np) == 0:
                            continue
                            
                        order = _np.argsort(-scores_np)[:k]
                        boxes_np = boxes_np[order]
                        scores_np = scores_np[order]
                        
                        # NMS
                        keep = self._nms_numpy(boxes_np, scores_np, iou_thr=nms_thr, max_num=max_num)
                        if len(keep) > 0:
                            boxes_np = boxes_np[keep]
                            scores_np = scores_np[keep]
                            boxes_all.append(boxes_np)
                            scores_all.append(scores_np)
                            
                    if len(boxes_all) == 0:
                        continue
                        
                    boxes_all = _np.concatenate(boxes_all, axis=0)
                    scores_all = _np.concatenate(scores_all, axis=0)
                    
                    # å†æ¬¡å…¨å±€ top-k
                    if len(boxes_all) > 0:
                        order = _np.argsort(-scores_all)[:max_num]
                        boxes_all = boxes_all[order]
                        b_col = _np.full((boxes_all.shape[0], 1), float(b), dtype=_np.float32)
                        rois_b = _np.concatenate([b_col, boxes_all.astype(_np.float32)], axis=1)
                        rois_concat.append(rois_b)
                        
            except Exception as e:
                print(f"âš ï¸  ç”Ÿæˆæè®®å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                
            if len(rois_concat) == 0:
                return jt.zeros((0, 5), dtype='float32')
                
            try:
                rois_np = _np.concatenate(rois_concat, axis=0)
                return jt.array(rois_np)
            except Exception as e:
                print(f"âš ï¸  æœ€ç»ˆæè®®åˆå¹¶å¤±è´¥: {e}")
                return jt.zeros((0, 5), dtype='float32')
        
        
    
    return JittorModelWithComponents(cfg, stage)


def create_jittor_optimizer(model, cfg):
    """åˆ›å»ºJittorä¼˜åŒ–å™¨"""
    print("ğŸ”§ åˆ›å»ºJittorä¼˜åŒ–å™¨...")
    
    # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–ä¼˜åŒ–å™¨è®¾ç½®
    optimizer_cfg = cfg.optimizer
    
    # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„å­¦ä¹ ç‡ï¼Œä¸å†å¼ºåˆ¶é™ä½
    base_lr = optimizer_cfg.get('lr', 0.02)
    
    print(f"ğŸ“Š ä½¿ç”¨å­¦ä¹ ç‡: {base_lr}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = None
    if optimizer_cfg.type == 'SGD':
        optimizer = jt.optim.SGD(
            model.parameters(),
            lr=base_lr,  # ç›´æ¥ä½¿ç”¨é…ç½®çš„å­¦ä¹ ç‡
            momentum=optimizer_cfg.get('momentum', 0.9),
            weight_decay=optimizer_cfg.get('weight_decay', 0.0001),
            nesterov=optimizer_cfg.get('nesterov', False)
        )
        print(f"âœ… åˆ›å»ºSGDä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡: {base_lr}")
    else:
        print(f"âš ï¸  ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_cfg.type}ï¼Œä½¿ç”¨é»˜è®¤SGD")
        optimizer = jt.optim.SGD(
            model.parameters(),
            lr=base_lr,
            momentum=0.9,
            weight_decay=0.0001
        )

    # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None  # é»˜è®¤å€¼
    if hasattr(cfg, 'lr_config') and cfg.lr_config is not None:
        lr_config = cfg.lr_config
        print(f"ğŸ“Š å­¦ä¹ ç‡é…ç½®: {lr_config}")
        
        # å¤„ç†stepç­–ç•¥ï¼ˆè¿™æ˜¯é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨çš„ç­–ç•¥ï¼‰
        if hasattr(lr_config, 'policy') and lr_config.policy == 'step':
            step = lr_config.get('step', [3, 4])
            gamma = lr_config.get('gamma', 0.1)
            if isinstance(step, list):
                milestones = step
            else:
                milestones = [step]
            
            scheduler = jt.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)
            print(f"âœ… è®¾ç½®StepLRè°ƒåº¦å™¨: milestones={milestones}, gamma={gamma}")
            
        elif hasattr(lr_config, 'type') and lr_config.type == 'MultiStepLR':
            milestones = lr_config.get('milestones', [8, 11])
            gamma = lr_config.get('gamma', 0.1)
            scheduler = jt.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)
            print(f"âœ… è®¾ç½®MultiStepLRè°ƒåº¦å™¨: milestones={milestones}, gamma={gamma}")
            
        elif hasattr(lr_config, 'type') and lr_config.type == 'StepLR':
            step_size = lr_config.get('step', 8)
            if isinstance(step_size, list):
                step_size = step_size[0] if len(step_size) > 0 else 8
            gamma = lr_config.get('gamma', 0.1)
            scheduler = jt.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
            print(f"âœ… è®¾ç½®StepLRè°ƒåº¦å™¨: step_size={step_size}, gamma={gamma}")
            
        else:
            if hasattr(lr_config, 'type'):
                print(f"âš ï¸  ä¸æ”¯æŒçš„å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹: {lr_config.type}")
            elif hasattr(lr_config, 'policy'):
                print(f"âš ï¸  ä¸æ”¯æŒçš„å­¦ä¹ ç‡ç­–ç•¥: {lr_config.policy}")
            else:
                print("âš ï¸  lr_configæ²¡æœ‰typeæˆ–policyå±æ€§")
            print("âš ï¸  å­¦ä¹ ç‡è°ƒåº¦å™¨æœªè®¾ç½®ï¼Œå°†ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡")
    else:
        print("âš ï¸  é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å­¦ä¹ ç‡é…ç½®ï¼Œå°†ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡")
    
    # ç¡®ä¿schedulerä¸ä¸ºNone
    if scheduler is None:
        print("ğŸ“Š åˆ›å»ºé»˜è®¤çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå›ºå®šå­¦ä¹ ç‡ï¼‰")
        scheduler = jt.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=1.0)  # å›ºå®šå­¦ä¹ ç‡
    
    return optimizer, scheduler


def create_jittor_trainer(model, datasets, cfg, args, distributed=False, validate=True, timestamp=None, meta=None, logger=None, json_log_path=None):
    """åˆ›å»ºJittorè®­ç»ƒå™¨"""
    print(f"ğŸ”§ åˆ›å»ºJittorè®­ç»ƒå™¨")
    if logger is None:
        from mmdet.utils import get_root_logger as _grl
        logger = _grl()
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    from mmdet.datasets import build_dataloader
    data_loaders = [
        build_dataloader(
            ds,
            cfg.data.samples_per_gpu,
            cfg.data.workers_per_gpu,
            num_gpus=1,
            dist=distributed,
            shuffle=True,
            seed=cfg.seed)
        for ds in datasets
    ]
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer, scheduler = create_jittor_optimizer(model, cfg)
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°ç¨³å®šæ€§
    print("ğŸ” æ£€æŸ¥æ¨¡å‹å‚æ•°ç¨³å®šæ€§...")
    try:
        param_stats = {}
        for name, param in model.named_parameters():
            if param.requires_grad:
                param_np = param.detach().numpy()
                param_stats[name] = {
                    'mean': float(np.mean(param_np)),
                    'std': float(np.std(param_np)),
                    'min': float(np.min(param_np)),
                    'max': float(np.max(param_np)),
                    'has_nan': np.any(np.isnan(param_np)),
                    'has_inf': np.any(np.isinf(param_np))
                }
                
                # æ£€æŸ¥å¼‚å¸¸å‚æ•°
                if param_stats[name]['has_nan']:
                    print(f"âš ï¸  å‚æ•° {name} åŒ…å« NaN å€¼")
                if param_stats[name]['has_inf']:
                    print(f"âš ï¸  å‚æ•° {name} åŒ…å« Inf å€¼")
                if abs(param_stats[name]['mean']) > 1000:
                    print(f"âš ï¸  å‚æ•° {name} å‡å€¼è¿‡å¤§: {param_stats[name]['mean']:.4f}")
        
        print(f"âœ… æ¨¡å‹å‚æ•°æ£€æŸ¥å®Œæˆï¼Œå…±æ£€æŸ¥ {len(param_stats)} ä¸ªå‚æ•°")
        
        # æ‰“å°å‰å‡ ä¸ªå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯
        print("ğŸ“Š å‰5ä¸ªå‚æ•°ç»Ÿè®¡:")
        for i, (name, stats) in enumerate(list(param_stats.items())[:5]):
            print(f"   {name}: mean={stats['mean']:.4f}, std={stats['std']:.4f}, range=[{stats['min']:.4f}, {stats['max']:.4f}]")
            
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹å‚æ•°æ£€æŸ¥å¤±è´¥: {e}")
    
    # æ¢¯åº¦è£å‰ªé…ç½®ï¼ˆæ¥è‡ª mmdet é…ç½®ï¼‰
    grad_clip_cfg = None
    try:
        if hasattr(cfg, 'optimizer_config') and cfg.optimizer_config is not None:
            grad_clip_cfg = getattr(cfg.optimizer_config, 'grad_clip', None)
    except Exception:
        grad_clip_cfg = None
    
    # å­¦ä¹ ç‡é…ç½®
    step_epochs = []
    if hasattr(cfg, 'lr_config'):
        lr_config = cfg.lr_config
        if hasattr(lr_config, 'type') and lr_config.type == 'MultiStepLR':
            step_epochs = lr_config.get('milestones', [])
            print(f"ğŸ“Š å­¦ä¹ ç‡è¡°å‡è½®æ¬¡: {step_epochs}")
        elif hasattr(lr_config, 'type') and lr_config.type == 'StepLR':
            step_size = lr_config.get('step', 8)
            if isinstance(step_size, list):
                step_epochs = step_size
            else:
                step_epochs = [step_size]
            print(f"ğŸ“Š å­¦ä¹ ç‡è¡°å‡è½®æ¬¡: {step_epochs}")
    
    # è®­ç»ƒå¾ªç¯
    # print("ğŸ¯ å¼€å§‹Jittorè®­ç»ƒå¾ªç¯...")
    logger.info("Start Jittor training loop...")
    max_epochs = args.epochs if hasattr(args, 'epochs') else (cfg.runner.max_epochs if hasattr(cfg, 'runner') else 12)
    
    # è®­ç»ƒç»Ÿè®¡
    total_steps = 0
    epoch_losses = []
    
    # JSON æ—¥å¿—å·¥å…·
    def append_json_log(record: dict):
        if not json_log_path:
            return
        try:
            with open(json_log_path, 'a') as jf:
                jf.write(json.dumps(record, ensure_ascii=False) + "\n")
        except Exception:
            pass

    # è®­ç»ƒå¼€å§‹æ—¥å¿—
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼")
    print(f"ğŸ“Š æ€»è½®æ¬¡: {max_epochs}")
    print(f"ğŸ“Š åˆå§‹å­¦ä¹ ç‡: {optimizer.lr:.6f}")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {cfg.data.samples_per_gpu}")
    print(f"ğŸ“Š å·¥ä½œç›®å½•: {cfg.work_dir}")
    logger.info(f"Training started: epochs={max_epochs}, lr={optimizer.lr:.6f}")
    
    # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
    epoch_records = []
    
    for epoch in range(max_epochs):
        print(f"\nğŸ“… è®­ç»ƒè½®æ¬¡ {epoch + 1}/{max_epochs}")
        print(f"ğŸ“Š å½“å‰å­¦ä¹ ç‡: {optimizer.lr:.6f}")
        logger.info(f"Epoch [{epoch+1}/{max_epochs}] lr={optimizer.lr:.6f}")
        
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        
        # è½®æ¬¡ç»Ÿè®¡
        epoch_loss = 0.0
        epoch_components = {}
        num_batches = 0
        
        # éå†æ•°æ®åŠ è½½å™¨
        total_batches = len(data_loaders[0])
        print(f"ğŸ“Š æœ¬è½®æ¬¡æ€»æ‰¹æ¬¡æ•°: {total_batches}")
        
        # æ·»åŠ æ‰¹æ¬¡è®¡æ•°å™¨ï¼Œç¡®ä¿å®é™…å¤„ç†äº†æ‰€æœ‰æ‰¹æ¬¡
        processed_batches = 0
        skipped_batches = 0
        
        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(
            enumerate(data_loaders[0]), 
            total=total_batches,
            desc=f"Epoch {epoch+1}/{max_epochs}",
            leave=True,
            ncols=100
        )
        
        for i, data_batch in pbar:
            
            try:
                # ä»…è½¬æ¢å¿…è¦é”®ï¼Œé¿å…å¯¹å¤æ‚å…ƒä¿¡æ¯é€’å½’å¯¼è‡´çš„ __instancecheck__ é€’å½’
                wanted_keys = ['img', 'gt_bboxes', 'gt_labels', 'proposals']
                jt_data = {}
                for key in wanted_keys:
                    if key in data_batch:
                        jt_data[key] = safe_convert_to_jittor(data_batch[key])

                # è¿›ä¸€æ­¥å¼ºåˆ¶ç±»å‹ä¸º Jittor Varï¼Œé¿å…æ··ç”¨ torch.Tensor
                def to_jt_var(x):
                    """å®‰å…¨åœ°å°†å„ç§æ•°æ®ç±»å‹è½¬æ¢ä¸ºJittor Var"""
                    try:
                        # å·²ç»æ˜¯Jittor Var
                        if isinstance(x, jt.Var):
                            return x
                        
                        # PyTorch Tensor
                        if hasattr(x, 'detach') and hasattr(x, 'cpu') and hasattr(x, 'numpy'):
                            try:
                                # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                                numpy_data = x.detach().cpu().numpy()
                                # è½¬æ¢ä¸ºfloat32ä»¥é¿å…ç²¾åº¦é—®é¢˜
                                if numpy_data.dtype != np.float32:
                                    numpy_data = numpy_data.astype(np.float32)
                                return jt.array(numpy_data)
                            except Exception as e:
                                print(f"âš ï¸  PyTorchè½¬æ¢å¤±è´¥: {e}")
                                return x
                        
                        # NumPy array
                        if hasattr(x, 'shape') and hasattr(x, 'dtype'):
                            try:
                                # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                                if x.dtype != np.float32:
                                    x = x.astype(np.float32)
                                return jt.array(x)
                            except Exception as e:
                                print(f"âš ï¸  NumPyè½¬æ¢å¤±è´¥: {e}")
                                return x
                        
                        # List of tensors
                        if isinstance(x, (list, tuple)) and len(x) > 0:
                            try:
                                converted_list = []
                                for item in x:
                                    converted_item = to_jt_var(item)
                                    converted_list.append(converted_item)
                                return converted_list
                            except Exception as e:
                                print(f"âš ï¸  åˆ—è¡¨è½¬æ¢å¤±è´¥: {e}")
                                return x
                        
                        return x
                    except Exception as e:
                        print(f"âš ï¸  to_jt_varè½¬æ¢å¤±è´¥: {e}")
                        return x

                # å¼ºåˆ¶è½¬æ¢æ‰€æœ‰æ•°æ®ä¸ºJittoræ ¼å¼
                if 'img' in jt_data:
                    jt_data['img'] = to_jt_var(jt_data['img'])
                    # ç¡®ä¿å›¾åƒæ•°æ®æ ¼å¼æ­£ç¡®
                    if hasattr(jt_data['img'], 'shape'):
                        print(f"ğŸ” å›¾åƒæ•°æ®è½¬æ¢å: {jt_data['img'].shape}, ç±»å‹: {type(jt_data['img'])}")
                
                if 'gt_bboxes' in jt_data and isinstance(jt_data['gt_bboxes'], (list, tuple)):
                    # å¤„ç†åµŒå¥—åˆ—è¡¨ç»“æ„
                    converted_bboxes = []
                    for v in jt_data['gt_bboxes']:
                        if isinstance(v, (list, tuple)):
                            # å¦‚æœæ˜¯åµŒå¥—åˆ—è¡¨ï¼Œé€’å½’è½¬æ¢
                            converted_bboxes.append([to_jt_var(item) for item in v])
                        else:
                            converted_bboxes.append(to_jt_var(v))
                    jt_data['gt_bboxes'] = converted_bboxes
                    
                    # æ£€æŸ¥è½¬æ¢ç»“æœ
                    for i, bbox in enumerate(jt_data['gt_bboxes']):
                        if isinstance(bbox, (list, tuple)) and len(bbox) > 0:
                            if hasattr(bbox[0], 'shape'):
                                print(f"ğŸ” GT bbox {i}: list with {len(bbox)} items, first item shape: {bbox[0].shape}, ç±»å‹: {type(bbox[0])}")
                            else:
                                print(f"ğŸ” GT bbox {i}: list with {len(bbox)} items, first item æ— shapeå±æ€§, ç±»å‹: {type(bbox[0])}")
                        elif hasattr(bbox, 'shape'):
                            print(f"ğŸ” GT bbox {i}: {bbox.shape}, ç±»å‹: {type(bbox)}")
                
                if 'gt_labels' in jt_data and isinstance(jt_data['gt_labels'], (list, tuple)):
                    # å¤„ç†åµŒå¥—åˆ—è¡¨ç»“æ„
                    converted_labels = []
                    for v in jt_data['gt_labels']:
                        if isinstance(v, (list, tuple)):
                            # å¦‚æœæ˜¯åµŒå¥—åˆ—è¡¨ï¼Œé€’å½’è½¬æ¢
                            converted_labels.append([to_jt_var(item) for item in v])
                        else:
                            converted_labels.append(to_jt_var(v))
                    jt_data['gt_labels'] = converted_labels
                    
                    # æ£€æŸ¥è½¬æ¢ç»“æœ
                    for i, label in enumerate(jt_data['gt_labels']):
                        if isinstance(label, (list, tuple)) and len(label) > 0:
                            if hasattr(label[0], 'shape'):
                                print(f"ğŸ” GT label {i}: list with {len(label)} items, first item shape: {label[0].shape}, ç±»å‹: {type(label[0])}")
                            else:
                                print(f"ğŸ” GT label {i}: list with {len(label)} items, first item æ— shapeå±æ€§, ç±»å‹: {type(label[0])}")
                        elif hasattr(label, 'shape'):
                            print(f"ğŸ” GT label {i}: {label.shape}, ç±»å‹: {type(label)}")
                
                if 'proposals' in jt_data:
                    # proposals å¯èƒ½æ˜¯ list æˆ– tensor
                    if isinstance(jt_data['proposals'], (list, tuple)):
                        jt_data['proposals'] = [to_jt_var(v) for v in jt_data['proposals']]
                    else:
                        jt_data['proposals'] = to_jt_var(jt_data['proposals'])
                
                # è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ˜¾ç¤ºï¼Œç®€åŒ–è¾“å‡ºï¼‰
                if i == 0:
                    print(f"ğŸ” æ•°æ®è°ƒè¯•ä¿¡æ¯:")
                    for key, value in jt_data.items():
                        if hasattr(value, 'shape'):
                            print(f"   {key}: {value.shape}, ç±»å‹: {type(value)}")
                        elif isinstance(value, (list, tuple)) and len(value) > 0:
                            print(f"   {key}: list with {len(value)} items")
                            if hasattr(value[0], 'shape'):
                                print(f"     first item shape: {value[0].shape}, ç±»å‹: {type(value[0])}")
                            else:
                                print(f"     first item æ— shapeå±æ€§, ç±»å‹: {type(value[0])}")
                        else:
                            print(f"   {key}: ç±»å‹: {type(value)}")
                
                # å‰å‘ä¼ æ’­
                losses = model(**jt_data)
                
                # ç«‹å³æ£€æŸ¥rcnn_score_lossï¼Œè¿™æ˜¯é—®é¢˜çš„æ ¹æº
                if isinstance(losses, dict) and 'rcnn_score_loss' in losses:
                    score_loss_val = losses['rcnn_score_loss'].item()
                    if abs(score_loss_val) > 1000:
                        print(f"ğŸš¨ æ£€æµ‹åˆ°å¼‚å¸¸çš„rcnn_score_loss: {score_loss_val}")
                        # ä¸è¦ç›´æ¥é‡ç½®ï¼Œè€Œæ˜¯å°è¯•ç¼©æ”¾
                        if score_loss_val > 0:
                            scale_factor = 1000.0 / score_loss_val
                            losses['rcnn_score_loss'] = losses['rcnn_score_loss'] * scale_factor
                            print(f"ğŸ”’ rcnn_score_loss å·²ç¼©æ”¾: {score_loss_val:.2e} -> {losses['rcnn_score_loss'].item():.4f}")
                        else:
                            # å¦‚æœæ˜¯è´Ÿå€¼ï¼Œå–ç»å¯¹å€¼åç¼©æ”¾
                            scale_factor = 1000.0 / abs(score_loss_val)
                            losses['rcnn_score_loss'] = losses['rcnn_score_loss'] * scale_factor
                            print(f"ğŸ”’ rcnn_score_loss å·²ç¼©æ”¾: {score_loss_val:.2e} -> {losses['rcnn_score_loss'].item():.4f}")
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æŸå¤±å€¼æ˜¯å¦åŒ…å« NaN æˆ– infï¼Œå¹¶è¿›è¡Œæ›´ä¸¥æ ¼çš„ç¨³å®šåŒ–å¤„ç†
                if isinstance(losses, dict):
                    # æ£€æŸ¥æ¯ä¸ªæŸå¤±å€¼å¹¶è¿›è¡Œç¨³å®šåŒ–å¤„ç†
                    for key, value in losses.items():
                        if hasattr(value, 'item'):
                            loss_val = value.item()
                            # æ›´ä¸¥æ ¼çš„æŸå¤±å€¼æ£€æŸ¥
                            if not np.isfinite(loss_val) or abs(loss_val) > 1000:
                                print(f"âš ï¸  WARNING: {key} = {loss_val} (å¼‚å¸¸å€¼)")
                                logger.warning(f"Abnormal loss detected: {key} = {loss_val}")
                                
                                # æ ¹æ®æŸå¤±ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
                                if key in ['rcnn_score_loss', 'rcnn_cls_loss', 'rcnn_bbox_loss']:
                                    # å¯¹äºåˆ†ç±»å’Œå›å½’æŸå¤±ï¼Œå°è¯•ç¼©æ”¾è€Œä¸æ˜¯é‡ç½®
                                    if np.isnan(loss_val) or np.isinf(loss_val):
                                        losses[key] = jt.array(0.1)
                                        print(f"ğŸ”’ {key} é‡ç½®ä¸º: 0.1 (NaN/Inf)")
                                    elif abs(loss_val) > 1000:
                                        # ç¼©æ”¾å¼‚å¸¸å¤§çš„æŸå¤±å€¼
                                        scale_factor = 100.0 / abs(loss_val)
                                        losses[key] = losses[key] * scale_factor
                                        print(f"ğŸ”’ {key} å·²ç¼©æ”¾: {loss_val:.2e} -> {losses[key].item():.4f}")
                                else:
                                    # å¯¹äºå…¶ä»–æŸå¤±ï¼Œå°è¯•ç¼©æ”¾
                                    if abs(loss_val) > 1000:
                                        scale_factor = 100.0 / abs(loss_val)
                                        losses[key] = losses[key] * scale_factor
                                        print(f"ğŸ”’ {key} å·²ç¼©æ”¾: {loss_val:.2e} -> {losses[key].item():.4f}")
                                    elif np.isnan(loss_val) or np.isinf(loss_val):
                                        losses[key] = jt.array(0.0)
                                        print(f"ğŸ”’ {key} é‡ç½®ä¸º: 0.0 (NaN/Inf)")
                    
                    # è®¡ç®—æ€»æŸå¤±å¹¶è¿›è¡Œç¨³å®šåŒ–
                    total_loss = sum(losses.values())
                    
                    # æ£€æŸ¥æ€»æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                    if hasattr(total_loss, 'item'):
                        total_loss_val = total_loss.item()
                        if not np.isfinite(total_loss_val) or abs(total_loss_val) > 1000:
                            print(f"âš ï¸  WARNING: æ€»æŸå¤± = {total_loss_val} (å¼‚å¸¸å€¼)")
                            logger.warning(f"Abnormal total loss: {total_loss_val}")
                            
                            # å¦‚æœæ€»æŸå¤±æ— æ•ˆï¼Œä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆæŸå¤±çš„æ€»å’Œ
                            valid_losses = []
                            for key, value in losses.items():
                                if hasattr(value, 'item'):
                                    val = value.item()
                                    if np.isfinite(val) and abs(val) <= 1000:
                                        valid_losses.append(value)
                            
                            if valid_losses:
                                total_loss = sum(valid_losses)
                                print(f"âœ… ä½¿ç”¨æœ‰æ•ˆæŸå¤±é‡æ–°è®¡ç®—æ€»æŸå¤±: {total_loss.item()}")
                            else:
                                # å¦‚æœæ‰€æœ‰æŸå¤±éƒ½æ— æ•ˆï¼Œå°è¯•ä½¿ç”¨ä¸€ä¸ªåŸºäºæ‰¹æ¬¡å¤§å°çš„åˆç†å€¼
                                total_loss = jt.array(0.1 * batch_size)
                                print(f"âš ï¸  æ‰€æœ‰æŸå¤±éƒ½æ— æ•ˆï¼Œä½¿ç”¨åŸºäºæ‰¹æ¬¡å¤§å°çš„å€¼: {total_loss.item()}")
                    
                    # ç´¯ç§¯å„é¡¹æŸå¤±
                    for key, value in losses.items():
                        if key != 'loss':
                            if key not in epoch_components:
                                epoch_components[key] = 0.0
                            try:
                                epoch_components[key] += value.item()
                            except Exception as e:
                                print(f"âš ï¸  ç´¯ç§¯æŸå¤±å¤±è´¥ {key}: {e}")
                                epoch_components[key] += 0.0
                else:
                    total_loss = losses
                    
                    # æ£€æŸ¥æ€»æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                    if hasattr(total_loss, 'item'):
                        total_loss_val = total_loss.item()
                        if not np.isfinite(total_loss_val) or abs(total_loss_val) > 1000:
                            print(f"âš ï¸  WARNING: æ€»æŸå¤± = {total_loss_val} (å¼‚å¸¸å€¼)")
                            logger.warning(f"Abnormal total loss: {total_loss_val}")
                            # å¦‚æœæ€»æŸå¤±æ— æ•ˆï¼Œä½¿ç”¨ä¸€ä¸ªå°çš„é»˜è®¤å€¼
                            total_loss = jt.array(0.001)
                
                # æ¸©å’Œåœ°é™åˆ¶æŸå¤±å€¼èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
                try:
                    if hasattr(total_loss, 'clamp'):
                        # å…ˆæ£€æŸ¥æŸå¤±å€¼æ˜¯å¦å¼‚å¸¸
                        loss_val = total_loss.item()
                        if not np.isfinite(loss_val):
                            print(f"âš ï¸  æ£€æµ‹åˆ°éæœ‰é™æŸå¤±å€¼: {loss_val}")
                            # å¦‚æœæŸå¤±å€¼éæœ‰é™ï¼Œä½¿ç”¨ä¸€ä¸ªåŸºäºæ‰¹æ¬¡å¤§å°çš„åˆç†å€¼
                            total_loss = jt.array(0.1 * batch_size)
                            print(f"ğŸ”’ ä½¿ç”¨åŸºäºæ‰¹æ¬¡å¤§å°çš„æŸå¤±å€¼: {total_loss.item()}")
                        elif abs(loss_val) > 10000:  # æé«˜é˜ˆå€¼ï¼Œé¿å…è¿‡åº¦é™åˆ¶
                            print(f"âš ï¸  æ£€æµ‹åˆ°è¿‡å¤§æŸå¤±å€¼: {loss_val}")
                            # å¦‚æœæŸå¤±å€¼è¿‡å¤§ï¼Œè¿›è¡Œæ¸©å’Œçš„ç¼©æ”¾
                            scale_factor = 1000.0 / abs(loss_val)
                            total_loss = total_loss * scale_factor
                            print(f"ğŸ”’ æŸå¤±å€¼å·²ç¼©æ”¾: {loss_val:.2e} -> {total_loss.item():.4f}")
                        else:
                            # åªåœ¨æŸå¤±å€¼æ­£å¸¸æ—¶è¿›è¡Œæ¸©å’Œé™åˆ¶
                            total_loss = total_loss.clamp(-1000.0, 1000.0)
                except Exception as e:
                    print(f"âš ï¸  æŸå¤±å€¼é™åˆ¶å¤±è´¥: {e}")
                    # å¦‚æœé™åˆ¶å¤±è´¥ï¼Œä½¿ç”¨åŸºäºæ‰¹æ¬¡å¤§å°çš„å€¼
                    total_loss = jt.array(0.1 * batch_size)
                    print(f"ğŸ”’ ä½¿ç”¨åŸºäºæ‰¹æ¬¡å¤§å°çš„æŸå¤±å€¼: {total_loss.item()}")
                
                # åå‘ä¼ æ’­ & æ¢¯åº¦è£å‰ªï¼ˆè‹¥é…ç½®å¯ç”¨ï¼‰
                # print(f"ğŸ”„ å¼€å§‹åå‘ä¼ æ’­...")
                grad_norm_value = None
                if grad_clip_cfg is not None:
                    # ä½¿ç”¨ jt.grad è®¡ç®—å…¨å±€æ¢¯åº¦èŒƒæ•°ï¼Œå¹¶æŒ‰éœ€ç¼©æ”¾ loss ä»¥ç­‰æ•ˆå®ç°è£å‰ª
                    try:
                        params = [p for p in model.parameters()]
                        grads = jt.grad(total_loss, params)
                        max_norm = float(getattr(grad_clip_cfg, 'max_norm', 20))
                        norm_type = float(getattr(grad_clip_cfg, 'norm_type', 2))
                        total_norm = 0.0
                        for g in grads:
                            if g is None:
                                continue
                            if norm_type == 2:
                                total_norm += float(jt.sum(g * g).item())
                            else:
                                total_norm += float(jt.sum(jt.abs(g) ** norm_type).item())
                        grad_norm_value = (total_norm ** 0.5) if norm_type == 2 else (total_norm ** (1.0 / norm_type))
                        if grad_norm_value > max_norm:
                            scale = max_norm / (grad_norm_value + 1e-6)
                            total_loss = total_loss * scale
                            print(f"âœ‚ï¸  æ¢¯åº¦è£å‰ª: åŸå§‹èŒƒæ•° {grad_norm_value:.4f}, è£å‰ªå {max_norm:.4f}")
                    except Exception:
                        pass
                
                # é¢å¤–çš„æ¢¯åº¦è£å‰ªä¿æŠ¤å’Œç›‘æ§
                try:
                    # è®¡ç®—æ¢¯åº¦å¹¶æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
                    params = [p for p in model.parameters()]
                    grads = jt.grad(total_loss, params)
                    
                    # è®¡ç®—æ¢¯åº¦èŒƒæ•°ç”¨äºç›‘æ§
                    grad_norm = 0.0
                    grad_has_nan = False
                    grad_has_inf = False
                    grad_has_zero = True  # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¢¯åº¦éƒ½ä¸º0
                    
                    for i, g in enumerate(grads):
                        if g is not None:
                            try:
                                g_np = g.numpy()
                                if np.any(np.isnan(g_np)):
                                    grad_has_nan = True
                                    print(f"âš ï¸  å‚æ•° {i} æ¢¯åº¦åŒ…å« NaN")
                                if np.any(np.isinf(g_np)):
                                    grad_has_inf = True
                                    print(f"âš ï¸  å‚æ•° {i} æ¢¯åº¦åŒ…å« Inf")
                                
                                # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                                g_norm = np.linalg.norm(g_np)
                                grad_norm += g_norm ** 2
                                
                                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ¥è¿‘0
                                if g_norm > 1e-8:
                                    grad_has_zero = False
                                    
                            except Exception:
                                pass
                    
                    grad_norm = grad_norm ** 0.5
                    
                    # æ¯100æ­¥æ‰“å°æ¢¯åº¦ä¿¡æ¯
                    if i % 100 == 0:
                        print(f"ğŸ“Š æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
                        if grad_has_zero:
                            print(f"âš ï¸  è­¦å‘Š: æ‰€æœ‰æ¢¯åº¦éƒ½æ¥è¿‘0ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒåœæ»")
                    
                    # å¦‚æœæ¢¯åº¦å¼‚å¸¸ï¼Œå°è¯•ä¿®å¤è€Œä¸æ˜¯è·³è¿‡
                    if grad_has_nan or grad_has_inf:
                        print(f"âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸æ¢¯åº¦ï¼Œå°è¯•ä¿®å¤...")
                        # å°è¯•ä½¿ç”¨ä¸€ä¸ªå°çš„å­¦ä¹ ç‡æ¥ç¨³å®šè®­ç»ƒ
                        if hasattr(optimizer, 'lr'):
                            original_lr = optimizer.lr
                            optimizer.lr = optimizer.lr * 0.1
                            print(f"ğŸ”’ ä¸´æ—¶é™ä½å­¦ä¹ ç‡: {original_lr:.6f} -> {optimizer.lr:.6f}")
                        
                        # ç»§ç»­è®­ç»ƒï¼Œè®©æ¨¡å‹å°è¯•æ¢å¤
                        print(f"ğŸ”„ ç»§ç»­è®­ç»ƒï¼Œå°è¯•æ¢å¤...")
                    
                    # å¦‚æœæ¢¯åº¦ä¸º0ï¼Œå°è¯•å¢åŠ æŸå¤±å€¼æ¥äº§ç”Ÿæ¢¯åº¦
                    if grad_has_zero and i % 50 == 0:
                        print(f"âš ï¸  æ£€æµ‹åˆ°æ¢¯åº¦ä¸º0ï¼Œå°è¯•å¢åŠ æŸå¤±å€¼...")
                        # è½»å¾®å¢åŠ æŸå¤±å€¼æ¥äº§ç”Ÿæ¢¯åº¦
                        total_loss = total_loss * 1.1
                        print(f"ğŸ”’ æŸå¤±å€¼å·²å¢åŠ : {total_loss.item():.6f}")
                        
                except Exception as e:
                    print(f"âš ï¸  æ¢¯åº¦æ£€æŸ¥å¤±è´¥: {e}")
                    # å¦‚æœæ¢¯åº¦è®¡ç®—å¤±è´¥ï¼Œå°è¯•ç»§ç»­è®­ç»ƒ
                    print(f"ğŸ”„ æ¢¯åº¦æ£€æŸ¥å¤±è´¥ï¼Œå°è¯•ç»§ç»­è®­ç»ƒ...")
                
                # æ›´æ–°å‚æ•°
                try:
                    optimizer.step(total_loss)
                    processed_batches += 1  # æˆåŠŸå¤„ç†çš„æ‰¹æ¬¡
                    # print(f"âœ… å‚æ•°æ›´æ–°æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸  ä¼˜åŒ–å™¨æ›´æ–°å¤±è´¥: {e}")
                    logger.error(f"Optimizer step failed: {e}")
                    # å¦‚æœä¼˜åŒ–å™¨æ›´æ–°å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ‰¹æ¬¡
                    skipped_batches += 1
                    continue
                
                # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if scheduler is not None:
                    try:
                        # æ£€æŸ¥å½“å‰å­¦ä¹ ç‡æ˜¯å¦è¿‡ä½
                        current_lr = optimizer.lr
                        if current_lr < 1e-6:  # å¦‚æœå­¦ä¹ ç‡è¿‡ä½ï¼Œé‡ç½®ä¸ºåˆå§‹å€¼
                            print(f"âš ï¸  å­¦ä¹ ç‡è¿‡ä½ ({current_lr:.2e})ï¼Œé‡ç½®ä¸ºåˆå§‹å€¼")
                            optimizer.lr = 0.005  # é‡ç½®ä¸ºåˆå§‹å­¦ä¹ ç‡
                        
                        scheduler.step()
                        
                        # å®‰å…¨åœ°è·å–å½“å‰å­¦ä¹ ç‡
                        if hasattr(optimizer, 'param_groups') and len(optimizer.param_groups) > 0:
                            current_lr = optimizer.param_groups[0].get('lr', 0.0)
                            if i % 200 == 0:  # æ¯200æ­¥æ‰“å°ä¸€æ¬¡å­¦ä¹ ç‡
                                print(f"ğŸ“Š å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")
                    except Exception as e:
                        print(f"âš ï¸  å­¦ä¹ ç‡è°ƒåº¦å™¨æ›´æ–°å¤±è´¥: {e}")
                        # å¦‚æœè°ƒåº¦å™¨æ›´æ–°å¤±è´¥ï¼Œå°è¯•é‡ç½®
                        try:
                            if hasattr(scheduler, 'reset'):
                                scheduler.reset()
                                print("ğŸ”„ å­¦ä¹ ç‡è°ƒåº¦å™¨å·²é‡ç½®")
                        except Exception as e2:
                            print(f"âš ï¸  å­¦ä¹ ç‡è°ƒåº¦å™¨é‡ç½®ä¹Ÿå¤±è´¥: {e2}")
                
                # ç´¯ç§¯æŸå¤±
                try:
                    if hasattr(total_loss, 'item'):
                        epoch_loss += total_loss.item()
                    else:
                        epoch_loss += float(total_loss)
                except Exception as e:
                    print(f"âš ï¸  ç´¯ç§¯æ€»æŸå¤±å¤±è´¥: {e}")
                    epoch_loss += 0.0
                
                num_batches += 1
                total_steps += 1

                # å‘¨æœŸæ€§å›æ”¶æ˜¾å­˜ï¼Œç¼“è§£ OOMï¼ˆJittor æ¨èï¼‰
                if (i + 1) % 200 == 0:
                    try:
                        jt.gc()
                    except Exception:
                        pass
            
                
                # æ›´æ–°tqdmè¿›åº¦æ¡æ˜¾ç¤ºæŸå¤±ä¿¡æ¯
                if isinstance(losses, dict):
                    # åªæ˜¾ç¤ºä¸»è¦çš„æŸå¤±å€¼ï¼Œé¿å…ä¿¡æ¯è¿‡å¤š
                    main_losses = {}
                    for k, v in losses.items():
                        if k in ['loss', 'rpn_cls_loss', 'rpn_bbox_loss', 'rcnn_cls_loss', 'rcnn_bbox_loss']:
                            try:
                                main_losses[k] = f"{v.item():.4f}"
                            except:
                                main_losses[k] = "0.0000"
                    
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    pbar.set_postfix({
                        'Loss': f"{total_loss.item():.4f}",
                        'RPN': f"{main_losses.get('rpn_cls_loss', '0.0000')}",
                        'RCNN': f"{main_losses.get('rcnn_cls_loss', '0.0000')}"
                    })
                else:
                    pbar.set_postfix({'Loss': f"{total_loss.item():.4f}"})
                
                # æ¯100æ­¥è®°å½•åˆ°loggerå’ŒJSONæ—¥å¿—
                if i % 100 == 0:
                    if isinstance(losses, dict):
                        loss_str = ', '.join([f'{k}: {v.item():.4f}' for k, v in losses.items()])
                    else:
                        loss_str = f'{total_loss.item():.4f}'
                    
                    # è®°å½•åˆ°logger
                    logger.info(f"Step {i+1}: {loss_str}")
                    
                    # JSON è¡Œæ—¥å¿—ï¼ˆä¸MMDeté£æ ¼æ¥è¿‘ï¼‰
                    record = {
                        'mode': 'train',
                        'epoch': epoch + 1,
                        'iter': i + 1,
                        'lr': float(optimizer.lr),
                        'total_batches': total_batches,
                        'grad_norm': float(grad_norm) if 'grad_norm' in locals() else 0.0,
                    }
                    if grad_norm_value is not None:
                        record['grad_norm'] = float(grad_norm_value)
                    if isinstance(losses, dict):
                        for k, v in losses.items():
                            try:
                                record[k] = float(v.item())
                            except Exception:
                                pass
                        record['loss'] = float(total_loss.item())
                    else:
                        record['loss'] = float(total_loss.item())
                    append_json_log(record)
                    
            except Exception as e:
                # åªåœ¨ç¬¬ä¸€ä¸ªé”™è¯¯æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œåç»­é”™è¯¯é™é»˜å¤„ç†
                if num_batches == 0:
                    print(f"âŒ æ‰¹æ¬¡ {i+1} å¤„ç†å¤±è´¥: {e}")
                    import traceback as _tb
                    _tb.print_exc()
                    print(f"   æ•°æ®ç±»å‹: {type(data_batch)}")
                    if isinstance(data_batch, dict):
                        for key, value in data_batch.items():
                            print(f"   {key}: {type(value)}")
                skipped_batches += 1
                continue
        
        # è®¡ç®—å¹³å‡æŸå¤±
        try:
            avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0
            # æ£€æŸ¥å¹³å‡æŸå¤±æ˜¯å¦æœ‰æ•ˆ
            if not np.isfinite(avg_loss):
                print(f"âš ï¸  WARNING: å¹³å‡æŸå¤± = {avg_loss} (éæœ‰é™å€¼)ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                avg_loss = 0.001
        except Exception as e:
            print(f"âš ï¸  è®¡ç®—å¹³å‡æŸå¤±å¤±è´¥: {e}")
            avg_loss = 0.001
        
        try:
            avg_components = {key: value / num_batches if num_batches > 0 else 0.0 
                             for key, value in epoch_components.items()}
            # æ£€æŸ¥ç»„ä»¶æŸå¤±æ˜¯å¦æœ‰æ•ˆ
            for key, value in avg_components.items():
                if not np.isfinite(value):
                    print(f"âš ï¸  WARNING: {key} = {value} (éæœ‰é™å€¼)ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    avg_components[key] = 0.0
        except Exception as e:
            print(f"âš ï¸  è®¡ç®—å¹³å‡ç»„ä»¶æŸå¤±å¤±è´¥: {e}")
            avg_components = {}
        
        epoch_losses.append(avg_loss)
        
        # å…³é—­tqdmè¿›åº¦æ¡
        pbar.close()
        
        print(f"\nğŸ“ˆ è½®æ¬¡ {epoch + 1} ç»Ÿè®¡:")
        print(f"   - å¹³å‡æ€»æŸå¤±: {avg_loss:.4f}")
        if avg_components:
            for key, value in avg_components.items():
                print(f"   - {key}: {value:.4f}")
        print(f"   - æ€»æ­¥æ•°: {total_steps}")
        print(f"   - æˆåŠŸå¤„ç†æ‰¹æ¬¡: {processed_batches}")
        print(f"   - è·³è¿‡æ‰¹æ¬¡: {skipped_batches}")
        print(f"   - å®é™…å¤„ç†æ‰¹æ¬¡: {num_batches}/{total_batches} ({num_batches/total_batches*100:.1f}%)")
        
        # è®°å½•åˆ°logger
        logger.info(
            f"Epoch {epoch+1} avg_loss={avg_loss:.4f}, steps={total_steps}, processed_batches={processed_batches}, skipped_batches={skipped_batches}"
        )
        
        # è®°å½•åˆ°JSONæ—¥å¿—
        epoch_record = {
            'mode': 'epoch_summary',
            'epoch': epoch + 1,
            'avg_loss': float(avg_loss),
            'total_steps': total_steps,
            'processed_batches': processed_batches,
            'skipped_batches': skipped_batches,
            'num_batches': num_batches,
            'total_batches': total_batches,
            'lr': float(optimizer.lr)
        }
        if avg_components:
            for key, value in avg_components.items():
                epoch_record[f'avg_{key}'] = float(value)
        append_json_log(epoch_record)
        
        # ä¿å­˜epochè®°å½•
        epoch_records.append(epoch_record)
        
        # å­¦ä¹ ç‡è¡°å‡
        if step_epochs and (epoch + 1) in step_epochs:
            old_lr = optimizer.lr
            optimizer.lr *= 0.1
            print(f"ğŸ“‰ å­¦ä¹ ç‡è¡°å‡: {old_lr:.6f} -> {optimizer.lr:.6f}")
        
        # éªŒè¯
        if validate and len(datasets) > 1:
            print(f"ğŸ” è¿›è¡ŒéªŒè¯...")
            model.eval()
        
        # æ˜¾ç¤ºå½“å‰epochå®ŒæˆçŠ¶æ€
        print(f"â±ï¸  Epoch {epoch+1} å®Œæˆæ—¶é—´: {datetime.datetime.now().strftime('%H:%M:%S')}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if hasattr(cfg, 'checkpoint_config') and cfg.checkpoint_config.interval > 0:
            if (epoch + 1) % cfg.checkpoint_config.interval == 0:
                # ç»Ÿä¸€ä½¿ç”¨ .pth æ‰©å±•åï¼Œä¾¿äºä¸ PyTorch æµç¨‹å¯¹é½
                checkpoint_path = osp.join(cfg.work_dir, f'epoch_{epoch+1}.pth')
                # å®é™…ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆJittor Var -> numpyï¼‰ï¼Œå¹¶åŒ…å«åŸºæœ¬å…ƒä¿¡æ¯
                try:
                    print(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ£€æŸ¥ç‚¹...")
                    
                    # å¼ºåˆ¶åŒæ­¥æ‰€æœ‰ CUDA æ“ä½œ
                    print(f"ğŸ”„ åŒæ­¥ CUDA æ“ä½œ...")
                    try:
                        jt.sync_all(True)
                        print(f"âœ… CUDA åŒæ­¥å®Œæˆ")
                    except Exception as e:
                        print(f"âš ï¸  CUDA åŒæ­¥è­¦å‘Š: {e}")
                    
                    # ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
                    import time
                    time.sleep(1)
                    
                    # è·å–æ¨¡å‹çŠ¶æ€
                    print(f"ğŸ“‹ è·å–æ¨¡å‹çŠ¶æ€...")
                    state = {}
                    try:
                        model_state = model.state_dict()
                        print(f"âœ… æ¨¡å‹çŠ¶æ€è·å–æˆåŠŸï¼ŒåŒ…å« {len(model_state)} ä¸ªå‚æ•°")
                    except Exception as e:
                        print(f"âŒ æ¨¡å‹çŠ¶æ€è·å–å¤±è´¥: {e}")
                        model_state = {}
                    
                    # è½¬æ¢å‚æ•°ä¸º numpy
                    print(f"ğŸ”„ è½¬æ¢å‚æ•°æ ¼å¼...")
                    for key, val in model_state.items():
                        try:
                            if hasattr(val, 'numpy'):
                                state[key] = val.numpy()
                            elif hasattr(val, 'detach') and hasattr(val, 'cpu'):
                                # å¤„ç†å¯èƒ½çš„ torch.Tensor
                                state[key] = val.detach().cpu().numpy()
                            else:
                                state[key] = val
                        except Exception as e:
                            print(f"âš ï¸  å‚æ•° {key} è½¬æ¢å¤±è´¥: {e}")
                            state[key] = val
                    
                    # å‡†å¤‡å…ƒä¿¡æ¯
                    meta = {
                        'epoch': epoch + 1,
                        'classes': getattr(model, 'CLASSES', None),
                        'config': getattr(cfg, 'pretty_text', None),
                        'timestamp': datetime.datetime.now().isoformat(),
                        'avg_loss': float(avg_loss),
                        'num_batches': num_batches
                    }
                    
                    # åˆ›å»ºç›®å½•å¹¶ä¿å­˜
                    print(f"ğŸ“ åˆ›å»ºä¿å­˜ç›®å½•...")
                    mmcv.mkdir_or_exist(osp.dirname(osp.abspath(checkpoint_path)))
                    
                    print(f"ğŸ’¾ å†™å…¥æ£€æŸ¥ç‚¹æ–‡ä»¶...")
                    with open(checkpoint_path, 'wb') as f:
                        pickle.dump({'state_dict': state, 'meta': meta}, f)
                    
                    print(f"âœ… æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {checkpoint_path}")
                    logger.info(f"Checkpoint saved to {checkpoint_path}")
                    
                except Exception as e:
                    print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                    logger.error(f"Failed to save checkpoint: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    # å°è¯•ä¿å­˜ä¸€ä¸ªç®€åŒ–çš„æ£€æŸ¥ç‚¹
                    try:
                        print(f"ğŸ”„ å°è¯•ä¿å­˜ç®€åŒ–æ£€æŸ¥ç‚¹...")
                        simple_checkpoint_path = osp.join(cfg.work_dir, f'epoch_{epoch+1}_simple.pth')
                        simple_state = {'epoch': epoch + 1, 'error': str(e)}
                        with open(simple_checkpoint_path, 'wb') as f:
                            pickle.dump(simple_state, f)
                        print(f"âœ… ç®€åŒ–æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {simple_checkpoint_path}")
                    except Exception as e2:
                        print(f"âŒ ç®€åŒ–æ£€æŸ¥ç‚¹ä¹Ÿä¿å­˜å¤±è´¥: {e2}")
        
        print(f"âœ… è½®æ¬¡ {epoch + 1} å®Œæˆ")
    
    # è®­ç»ƒå®Œæˆæ€»ç»“
    print(f"\nğŸ‰ Jittorè®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
    print(f"   - æ€»è½®æ¬¡: {max_epochs}")
    print(f"   - æœ€ç»ˆå¹³å‡æŸå¤±: {np.mean(epoch_losses):.4f}")
    print(f"   - æ€»æ­¥æ•°: {total_steps}")
    print(f"   - æ€»æˆåŠŸæ‰¹æ¬¡: {sum([epoch_record.get('processed_batches', 0) for epoch_record in epoch_records if 'processed_batches' in epoch_record])}")
    print(f"   - æ€»è·³è¿‡æ‰¹æ¬¡: {sum([epoch_record.get('skipped_batches', 0) for epoch_record in epoch_records if 'skipped_batches' in epoch_record])}")
    
    # è®°å½•åˆ°logger
    logger.info(f"Training completed: epochs={max_epochs}, final_avg_loss={np.mean(epoch_losses):.4f}, total_steps={total_steps}")
    
    # è®°å½•åˆ°JSONæ—¥å¿—
    final_record = {
        'mode': 'training_complete',
        'total_epochs': max_epochs,
        'final_avg_loss': float(np.mean(epoch_losses)),
        'total_steps': total_steps,
        'timestamp': datetime.datetime.now().isoformat()
    }
    append_json_log(final_record)


def main():
    args = parse_args()

    print("=" * 60)
    print(f"ğŸ”¬ æ··åˆæ¨¡å¼è®­ç»ƒ - Jittor + MMDetection")
    print("=" * 60)

    # åŠ è½½è‡ªå®šä¹‰ç»„ä»¶
    if not load_custom_components():
        print("âŒ è‡ªå®šä¹‰ç»„ä»¶åŠ è½½å¤±è´¥ï¼Œé€€å‡º")
        return

    # è®¾ç½®Jittor
    setup_jittor()

    cfg = Config.fromfile(args.config)
    if args.cfg_options is not None:
        cfg.merge_from_dict(args.cfg_options)
    
    # å¯¼å…¥å­—ç¬¦ä¸²åˆ—è¡¨ä¸­çš„æ¨¡å—
    if cfg.get('custom_imports', None):
        from mmcv.utils import import_modules_from_strings
        import_modules_from_strings(**cfg['custom_imports'])

    # work_dirçš„ä¼˜å…ˆçº§ï¼šCLI > æ–‡ä»¶ä¸­çš„æ®µ > æ–‡ä»¶å
    if args.work_dir is not None:
        cfg.work_dir = args.work_dir
    elif cfg.get('work_dir', None) is None:
        cfg.work_dir = osp.join('./work_dirs', osp.splitext(osp.basename(args.config))[0])
    if args.resume_from is not None:
        cfg.resume_from = args.resume_from
    if args.gpu_ids is not None:
        cfg.gpu_ids = args.gpu_ids
    else:
        cfg.gpu_ids = range(1) if args.gpus is None else range(args.gpus)

    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    if args.launcher == 'none':
        distributed = False
    else:
        distributed = True
        init_dist(args.launcher, **cfg.dist_params)
        _, world_size = get_dist_info()
        cfg.gpu_ids = range(world_size)

    # åˆ›å»ºwork_dirï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„æ··æ·†ï¼‰
    abs_work_dir = osp.abspath(cfg.work_dir)
    mmcv.mkdir_or_exist(abs_work_dir)
    cfg.work_dir = abs_work_dir
    cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))
    
    # åˆå§‹åŒ–logger
    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
    log_file = osp.join(cfg.work_dir, f'{timestamp}_mixed_mode.log')
    log_level = cfg.get('log_level', 'INFO')
    logger = get_root_logger(log_file=log_file, log_level=log_level)
    # è¿½åŠ ä¸€ä¸ª json è¡Œæ—¥å¿—æ–‡ä»¶ï¼Œå…¼å®¹ mmdet é£æ ¼
    json_log_path = osp.join(cfg.work_dir, f'{timestamp}.log.json')

    # åˆå§‹åŒ–meta dict
    meta = dict()
    env_info_dict = collect_env()
    env_info = '\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])
    dash_line = '-' * 60 + '\n'
    logger.info('ç¯å¢ƒä¿¡æ¯:\n' + dash_line + env_info + '\n' + dash_line)
    meta['env_info'] = env_info
    meta['config'] = cfg.pretty_text
    logger.info(f'åˆ†å¸ƒå¼è®­ç»ƒ: {distributed}')
    logger.info(f'å¼€å§‹è®­ç»ƒ')
    logger.info(f'é…ç½®:\n{cfg.pretty_text}')

    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        logger.info(f'è®¾ç½®éšæœºç§å­ä¸º {args.seed}, ç¡®å®šæ€§: {args.deterministic}')
        set_random_seed(args.seed, deterministic=args.deterministic)
    cfg.seed = args.seed
    meta['seed'] = args.seed
    meta['exp_name'] = osp.basename(args.config)
    meta['stage'] = 'auto_detected'

    # æ„å»ºæ•°æ®é›†
    print("ğŸ“Š æ„å»ºæ•°æ®é›†...")
    datasets = [build_dataset(cfg.data.train)]
    workflow = cfg.get('workflow', [('train', 1)])
    if len(workflow) == 2:
        val_dataset = copy.deepcopy(cfg.data.val)
        val_dataset.pipeline = cfg.data.train.pipeline
        datasets.append(build_dataset(val_dataset))
    # è¾“å‡ºè®­ç»ƒé›†å›¾åƒæ•°é‡ï¼Œä¾¿äºç¡®è®¤æ­¥æ•°
    try:
        train_len = len(datasets[0])
        print(f"ğŸ“¦ è®­ç»ƒé›†å›¾åƒæ•°é‡: {train_len}")
        logger.info(f"Train dataset length (images): {train_len}")
    except Exception:
        pass
    
    model_classes = datasets[0].CLASSES

    # è‡ªåŠ¨æ£€æµ‹è®­ç»ƒé˜¶æ®µ
    stage = detect_training_stage(cfg, args.config)
    
    # åˆ›å»ºJittorå…¼å®¹çš„æ¨¡å‹
    model = create_jittor_compatible_model(cfg, stage)
    model.CLASSES = model_classes

    # ä½¿ç”¨è‡ªå®šä¹‰çš„Jittorè®­ç»ƒå™¨
    try:
        create_jittor_trainer(
            model,
            datasets,
            cfg,
            args,
            distributed=distributed,
            validate=(not args.no_validate),
            timestamp=timestamp,
            meta=meta,
            logger=logger,
            json_log_path=json_log_path)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

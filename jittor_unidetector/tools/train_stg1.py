import argparse
import copy
import os
import os.path as osp
import time
import warnings
import sys
import numpy as np
import json
import pickle
import datetime
from tqdm import tqdm

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥utilsæ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

import jittor as jt
import jittor.models as jm
import mmcv
from mmcv import Config, DictAction
from mmcv.runner import get_dist_info, init_dist
from mmcv.utils import get_git_hash

from mmdet import __version__
from mmdet.apis import set_random_seed
from mmdet.datasets import build_dataset

from mmdet.models.builder import build_head, build_roi_extractor, build_neck
from mmdet.utils import collect_env, get_root_logger

from utils.train_utils import setup_jittor, clear_jittor_cache, detect_training_stage
from utils.data_util import ensure_jittor_var, safe_convert_to_jittor, safe_sum
from jittor_components.JittorModel import create_jittor_compatible_model
from jittor_components.JittorOptimizer import create_jittor_optimizer
from jittor_components.JittorTrainer import create_jittor_trainer


def parse_args():
    parser = argparse.ArgumentParser(description='æ··åˆæ¨¡å¼è®­ç»ƒæ£€æµ‹å™¨')
    parser.add_argument('config', help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--work-dir', help='ä¿å­˜æ—¥å¿—å’Œæ¨¡å‹çš„ç›®å½•')
    parser.add_argument('--resume-from', help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹æ–‡ä»¶')
    parser.add_argument('--no-validate', action='store_true', help='è®­ç»ƒæœŸé—´ä¸è¯„ä¼°æ£€æŸ¥ç‚¹')
    parser.add_argument('--epochs', type=int, default=4, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=2, help='æ‰¹æ¬¡å¤§å°')
    # ç§»é™¤æ‰‹åŠ¨stageå‚æ•°ï¼Œæ”¹ä¸ºè‡ªåŠ¨è¯†åˆ«
    group_gpus = parser.add_mutually_exclusive_group()
    group_gpus.add_argument('--gpus', type=int, help='ä½¿ç”¨çš„GPUæ•°é‡')
    group_gpus.add_argument('--gpu-ids', type=int, nargs='+', help='ä½¿ç”¨çš„GPU ID')
    parser.add_argument('--seed', type=int, default=None, help='éšæœºç§å­')
    parser.add_argument('--deterministic', action='store_true', help='æ˜¯å¦ä¸ºCUDNNåç«¯è®¾ç½®ç¡®å®šæ€§é€‰é¡¹')
    parser.add_argument('--options', nargs='+', action=DictAction, help='è¦†ç›–é…ç½®è®¾ç½®')
    parser.add_argument('--cfg-options', nargs='+', action=DictAction, help='è¦†ç›–é…ç½®è®¾ç½®')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm', 'mpi'], default='none', help='ä½œä¸šå¯åŠ¨å™¨')
    parser.add_argument('--local_rank', type=int, default=0)
    args = parser.parse_args()
    if 'LOCAL_RANK' not in os.environ:
        os.environ['LOCAL_RANK'] = str(args.local_rank)
    if args.options and args.cfg_options:
        raise ValueError('--options å’Œ --cfg-options ä¸èƒ½åŒæ—¶æŒ‡å®š')
    if args.options:
        warnings.warn('--options å·²è¢«å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ --cfg-options')
        args.cfg_options = args.options
    return args


def load_custom_components():
    """åŠ è½½è‡ªå®šä¹‰ç»„ä»¶"""
    print("ğŸ“¦ åŠ è½½è‡ªå®šä¹‰ç»„ä»¶...")
    try:
        # æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥modelsæ¨¡å—
        current_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(current_dir)
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)
        
        # å¯¼å…¥æ‰€æœ‰Jittoræ¨¡å‹ç»„ä»¶ï¼Œç¡®ä¿å®ƒä»¬è¢«æ³¨å†Œåˆ°mmdetæ³¨å†Œè¡¨ä¸­
        from models.backbones.clip_backbone import CLIPResNet
        from models.heads.roi_heads.oln_roi_head import OlnRoIHead
        from models.heads.roi_heads.bbox_heads.bbox_head_clip_partitioned import BBoxHeadCLIPPartitioned
        from models.heads.roi_heads.bbox_heads.shared2fc_bbox_score_head import Shared2FCBBoxScoreHead
        from models.heads.rpn_head import RPNHead
        from models.heads.oln_rpn_head import OlnRPNHead
        from models.necks.fpn import FPN
        from models.detectors.faster_rcnn import FasterRCNN
        from models.detectors.fast_rcnn import FastRCNN
        from models.heads.roi_heads.roi_extractors.single_roi_extractor import SingleRoIExtractor
        
        print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰Jittoræ¨¡å‹ç»„ä»¶")
        return True
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å‹ç»„ä»¶å¤±è´¥: {e}")
        return False



def main():
    args = parse_args()

    print("=" * 60)
    print(f"ğŸ”¬ æ··åˆæ¨¡å¼è®­ç»ƒ - Jittor + MMDetection")
    print("=" * 60)

    # åŠ è½½è‡ªå®šä¹‰ç»„ä»¶
    if not load_custom_components():
        print("âŒ è‡ªå®šä¹‰ç»„ä»¶åŠ è½½å¤±è´¥ï¼Œé€€å‡º")
        return

    # è®¾ç½®Jittor
    setup_jittor()

    cfg = Config.fromfile(args.config)
    if args.cfg_options is not None:
        cfg.merge_from_dict(args.cfg_options)
    
    # è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°ä»¥é˜²æ­¢å†…å­˜æº¢å‡º
    if hasattr(cfg, 'data') and hasattr(cfg.data, 'samples_per_gpu'):
        original_batch_size = cfg.data.samples_per_gpu
        # å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äº1ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º1ï¼ˆè¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
        if original_batch_size > 1:
            cfg.data.samples_per_gpu = 1
            print(f"âš ï¸  è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°: {original_batch_size} -> {cfg.data.samples_per_gpu} (é˜²æ­¢å†…å­˜æº¢å‡º)")
    
    # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†æ‰¹æ¬¡å¤§å°ï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    if args.batch_size:
        if hasattr(cfg, 'data'):
            cfg.data.samples_per_gpu = args.batch_size
            print(f"ğŸ“ ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    
    # å¯¼å…¥å­—ç¬¦ä¸²åˆ—è¡¨ä¸­çš„æ¨¡å—
    if cfg.get('custom_imports', None):
        from mmcv.utils import import_modules_from_strings
        import_modules_from_strings(**cfg['custom_imports'])

    # work_dirçš„ä¼˜å…ˆçº§ï¼šCLI > æ–‡ä»¶ä¸­çš„æ®µ > æ–‡ä»¶å
    if args.work_dir is not None:
        cfg.work_dir = args.work_dir
    elif cfg.get('work_dir', None) is None:
        cfg.work_dir = osp.join('./work_dirs', osp.splitext(osp.basename(args.config))[0])
    if args.resume_from is not None:
        cfg.resume_from = args.resume_from
    if args.gpu_ids is not None:
        cfg.gpu_ids = args.gpu_ids
    else:
        cfg.gpu_ids = range(1) if args.gpus is None else range(args.gpus)

    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    if args.launcher == 'none':
        distributed = False
    else:
        distributed = True
        init_dist(args.launcher, **cfg.dist_params)
        _, world_size = get_dist_info()
        cfg.gpu_ids = range(world_size)

    # åˆ›å»ºwork_dirï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„æ··æ·†ï¼‰
    abs_work_dir = osp.abspath(cfg.work_dir)
    mmcv.mkdir_or_exist(abs_work_dir)
    cfg.work_dir = abs_work_dir
    cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))
    
    # åˆå§‹åŒ–logger
    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
    log_file = osp.join(cfg.work_dir, f'{timestamp}_mixed_mode.log')
    log_level = cfg.get('log_level', 'INFO')
    logger = get_root_logger(log_file=log_file, log_level=log_level)
    # è¿½åŠ ä¸€ä¸ª json è¡Œæ—¥å¿—æ–‡ä»¶ï¼Œå…¼å®¹ mmdet é£æ ¼
    json_log_path = osp.join(cfg.work_dir, f'{timestamp}.log.json')

    # åˆå§‹åŒ–meta dict
    meta = dict()
    env_info_dict = collect_env()
    env_info = '\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])
    dash_line = '-' * 60 + '\n'
    logger.info('ç¯å¢ƒä¿¡æ¯:\n' + dash_line + env_info + '\n' + dash_line)
    meta['env_info'] = env_info
    meta['config'] = cfg.pretty_text
    logger.info(f'åˆ†å¸ƒå¼è®­ç»ƒ: {distributed}')
    logger.info(f'å¼€å§‹è®­ç»ƒ')
    logger.info(f'é…ç½®:\n{cfg.pretty_text}')

    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        logger.info(f'è®¾ç½®éšæœºç§å­ä¸º {args.seed}, ç¡®å®šæ€§: {args.deterministic}')
        set_random_seed(args.seed, deterministic=args.deterministic)
    cfg.seed = args.seed
    meta['seed'] = args.seed
    meta['exp_name'] = osp.basename(args.config)
    meta['stage'] = 'auto_detected'

    # æ„å»ºæ•°æ®é›†
    print("ğŸ“Š æ„å»ºæ•°æ®é›†...")
    datasets = [build_dataset(cfg.data.train)]
    workflow = cfg.get('workflow', [('train', 1)])
    if len(workflow) == 2:
        val_dataset = copy.deepcopy(cfg.data.val)
        val_dataset.pipeline = cfg.data.train.pipeline
        datasets.append(build_dataset(val_dataset))
    # è¾“å‡ºè®­ç»ƒé›†å›¾åƒæ•°é‡ï¼Œä¾¿äºç¡®è®¤æ­¥æ•°
    try:
        train_len = len(datasets[0])
        print(f"ğŸ“¦ è®­ç»ƒé›†å›¾åƒæ•°é‡: {train_len}")
        logger.info(f"Train dataset length (images): {train_len}")
    except Exception:
        pass
    
    model_classes = datasets[0].CLASSES

    # è‡ªåŠ¨æ£€æµ‹è®­ç»ƒé˜¶æ®µ
    stage = detect_training_stage(cfg, args.config)
    
    # åˆ›å»ºJittorå…¼å®¹çš„æ¨¡å‹
    model = create_jittor_compatible_model(cfg, stage)
    model.CLASSES = model_classes

    # ä½¿ç”¨è‡ªå®šä¹‰çš„Jittorè®­ç»ƒå™¨
    try:
        create_jittor_trainer(
            model,
            datasets,
            cfg,
            args,
            distributed=distributed,
            validate=(not args.no_validate),
            timestamp=timestamp,
            meta=meta,
            logger=logger,
            json_log_path=json_log_path)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

import jittor as jt
import numpy as np
import os
import gc
import time

class AverageMeter:
    """è®¡ç®—å’Œå­˜å‚¨å¹³å‡å€¼å’Œå½“å‰å€¼"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0
    
    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def save_checkpoint(model, optimizer, epoch, filename):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    state = {
        'epoch': epoch,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }
    jt.save(state, filename)
    print(f"Checkpoint saved to {filename}")

def load_checkpoint(model, optimizer, filename):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    if os.path.exists(filename):
        checkpoint = jt.load(filename)
        model.load_state_dict(checkpoint['model'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        epoch = checkpoint['epoch']
        print(f"Checkpoint loaded from {filename}, epoch {epoch}")
        return epoch
    else:
        print(f"Checkpoint {filename} not found")
        return 0

def adjust_learning_rate(optimizer, epoch, lr, schedule):
    """è°ƒæ•´å­¦ä¹ ç‡"""
    if epoch in schedule:
        lr *= 0.1
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        print(f"Learning rate adjusted to {lr}")

def accuracy(output, target, topk=(1,)):
    """è®¡ç®—top-kå‡†ç¡®ç‡"""
    maxk = max(topk)
    batch_size = target.shape[0]
    
    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.reshape(1, -1).expand_as(pred))
    
    res = []
    for k in topk:
        correct_k = correct[:k].reshape(-1).float().sum(0)
        res.append(correct_k.mul_(100.0 / batch_size))
    return res

class Timer:
    """è®¡æ—¶å™¨"""
    
    def __init__(self):
        self.start_time = None
        self.end_time = None
    
    def start(self):
        self.start_time = time.time()
    
    def end(self):
        self.end_time = time.time()
        return self.end_time - self.start_time
    
    def get_time(self):
        if self.start_time is None:
            return 0
        if self.end_time is None:
            return time.time() - self.start_time
        return self.end_time - self.start_time

def create_logger(log_file):
    """åˆ›å»ºæ—¥å¿—è®°å½•å™¨"""
    import logging
    
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # æ–‡ä»¶å¤„ç†å™¨
    fh = logging.FileHandler(log_file)
    fh.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    # æ·»åŠ å¤„ç†å™¨
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger

def set_random_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    jt.set_global_seed(seed)

def setup_jittor():
    """è®¾ç½®Jittorç¯å¢ƒ"""
    try:
        import jittor as jt
        
        # è®¾ç½®JittoråŸºæœ¬é…ç½®
        jt.flags.use_cuda = 1  # å¯ç”¨CUDA
        
        # è®¾ç½®å†…å­˜ä¼˜åŒ–é€‰é¡¹ï¼ˆä½¿ç”¨æ”¯æŒçš„æ ‡å¿—ï¼‰
        if hasattr(jt.flags, 'amp_level'):
            jt.flags.amp_level = 0  # ç¦ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆå¯èƒ½å¯¼è‡´å†…å­˜é—®é¢˜ï¼‰
        
        if hasattr(jt.flags, 'lazy_execution'):
            jt.flags.lazy_execution = 0  # ç¦ç”¨å»¶è¿Ÿæ‰§è¡Œï¼ˆå¯èƒ½å¯¼è‡´å†…å­˜ç´¯ç§¯ï¼‰
        
        # è®¾ç½®å†…å­˜æ¸…ç†é¢‘ç‡ï¼ˆä½¿ç”¨æ”¯æŒçš„æ ‡å¿—ï¼‰
        if hasattr(jt.flags, 'gc_after_backward'):
            jt.flags.gc_after_backward = 1  # åå‘ä¼ æ’­åè‡ªåŠ¨åƒåœ¾å›æ”¶
        
        if hasattr(jt.flags, 'gc_after_forward'):
            jt.flags.gc_after_forward = 1  # å‰å‘ä¼ æ’­åä¹Ÿè‡ªåŠ¨åƒåœ¾å›æ”¶
        
        # è®¾ç½®å†…å­˜é™åˆ¶ï¼ˆé˜²æ­¢GPUå†…å­˜æº¢å‡ºï¼‰
        # æ³¨æ„ï¼šæŸäº›ç‰ˆæœ¬çš„Jittorå¯èƒ½ä¸æ”¯æŒmax_memoryæ ‡å¿—
        try:
            if hasattr(jt.flags, 'max_memory'):
                jt.flags.max_memory = "12GB"  # æ›´æ¿€è¿›åœ°é™åˆ¶æœ€å¤§å†…å­˜ä½¿ç”¨
                print(f"ğŸ’¾ è®¾ç½®æœ€å¤§å†…å­˜é™åˆ¶: 12GB")
        except:
            print("âš ï¸  max_memoryæ ‡å¿—ä¸æ”¯æŒï¼Œä½¿ç”¨å…¶ä»–å†…å­˜ç®¡ç†ç­–ç•¥")
        
        # è®¾ç½®å…¶ä»–å†…å­˜ä¼˜åŒ–æ ‡å¿—
        try:
            if hasattr(jt.flags, 'memory_efficient'):
                jt.flags.memory_efficient = 1  # å¯ç”¨å†…å­˜æ•ˆç‡æ¨¡å¼
                print(f"ğŸ’¾ å¯ç”¨å†…å­˜æ•ˆç‡æ¨¡å¼")
        except:
            pass
        
        try:
            if hasattr(jt.flags, 'use_parallel_op'):
                jt.flags.use_parallel_op = 0  # ç¦ç”¨å¹¶è¡Œæ“ä½œä»¥å‡å°‘å†…å­˜ä½¿ç”¨
                print(f"ğŸ’¾ ç¦ç”¨å¹¶è¡Œæ“ä½œä»¥å‡å°‘å†…å­˜ä½¿ç”¨")
        except:
            pass
        
        print(f"âœ… Jittorè®¾ç½®å®Œæˆ")
        print(f"ğŸ® CUDA: {jt.flags.use_cuda}")
        print(f"ğŸ§¹ è‡ªåŠ¨æ¸…ç†: åå‘ä¼ æ’­å={jt.flags.gc_after_backward if hasattr(jt.flags, 'gc_after_backward') else 'N/A'}, å‰å‘ä¼ æ’­å={jt.flags.gc_after_forward if hasattr(jt.flags, 'gc_after_forward') else 'N/A'}")
        print(f"ğŸ’¾ å†…å­˜é™åˆ¶: {jt.flags.max_memory if hasattr(jt.flags, 'max_memory') else 'N/A'}")
        
        return jt
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥Jittorï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…")
        return None

def clear_jittor_cache():
    """æ¸…ç†Jittorç¼“å­˜"""
    try:
        if hasattr(jt, 'core') and hasattr(jt.core, 'clear_cache'):
            jt.core.clear_cache()
        gc.collect()
    except:
        pass

def get_model_info(model):
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    info = {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'model_size_mb': total_params * 4 / 1024 / 1024  # å‡è®¾float32
    }
    
    return info 

def detect_training_stage(cfg, config_path):
    """è‡ªåŠ¨æ£€æµ‹è®­ç»ƒé˜¶æ®µ"""
    
    model_type = cfg.model.type
    if model_type == 'FasterRCNN':
        stage = '1st'
    elif model_type == 'FastRCNN':
        stage = '2nd'
    
    print(f"æ£€æµ‹åˆ°è®­ç»ƒé˜¶æ®µ: {stage}")
    return stage
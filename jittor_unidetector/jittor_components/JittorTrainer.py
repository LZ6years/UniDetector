import jittor as jt
import numpy as np
import os
import os.path as osp
import sys
import gc
import pickle
import datetime
import time
import warnings
import json
from tqdm import tqdm

# æ·»åŠ UniDetectorçš„mmdetè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../..'))
import mmcv
from mmdet.datasets import build_dataloader

from utils.data_util import ensure_jittor_var, safe_convert_to_jittor, safe_sum
from utils.train_utils import clear_jittor_cache
from jittor_components.JittorOptimizer import create_jittor_optimizer

def create_jittor_trainer(model, datasets, cfg, args, distributed=False, validate=True, timestamp=None, meta=None, logger=None, json_log_path=None):
    """åˆ›å»ºJittorè®­ç»ƒå™¨"""
    print(f"åˆ›å»ºJittorè®­ç»ƒå™¨")
    if logger is None:
        from mmdet.utils import get_root_logger as _grl
        logger = _grl()
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    from mmdet.datasets import build_dataloader
    
    # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨åŒ…è£…å™¨ï¼Œå¤„ç†DataContainer
    def create_jittor_dataloader(dataset, samples_per_gpu, workers_per_gpu, **kwargs):
        """åˆ›å»ºJittorå…¼å®¹çš„æ•°æ®åŠ è½½å™¨"""
        dataloader = build_dataloader(
            dataset,
            samples_per_gpu,
            workers_per_gpu,
            num_gpus=1,
            dist=distributed,
            shuffle=True,
            seed=cfg.seed
        )
        
        # åŒ…è£…æ•°æ®åŠ è½½å™¨ï¼Œåœ¨è¿”å›æ•°æ®æ—¶å¤„ç†DataContainer
        class JittorDataLoaderWrapper:
            def __init__(self, original_loader):
                self.original_loader = original_loader
                self.dataset = original_loader.dataset
                self.batch_size = original_loader.batch_size
                self.num_workers = original_loader.num_workers
                self.sampler = original_loader.sampler
                self.pin_memory = getattr(original_loader, 'pin_memory', False)
                self.drop_last = getattr(original_loader, 'drop_last', False)
                self.timeout = getattr(original_loader, 'timeout', 0)
                self.worker_init_fn = getattr(original_loader, 'worker_init_fn', None)
                self.multiprocessing_context = getattr(original_loader, 'multiprocessing_context', None)
                self.generator = getattr(original_loader, 'generator', None)
                self.prefetch_factor = getattr(original_loader, 'prefetch_factor', 2)
                self.persistent_workers = getattr(original_loader, 'persistent_workers', False)
            
            def __iter__(self):
                for batch in self.original_loader:
                    # é¢„å¤„ç†æ•°æ®ï¼Œæå–DataContainerä¸­çš„æ•°æ®
                    processed_batch = {}
                    for key, value in batch.items():
                        if hasattr(value, 'data') and hasattr(value, 'stack') and hasattr(value, 'cpu_only'):
                            # è¿™æ˜¯DataContainerï¼Œæå–å…¶dataå±æ€§
                            processed_batch[key] = value.data
                        else:
                            processed_batch[key] = value
                    yield processed_batch
            
            def __len__(self):
                return len(self.original_loader)
        
        return JittorDataLoaderWrapper(dataloader)
    
    data_loaders = [
        create_jittor_dataloader(
            ds,
            cfg.data.samples_per_gpu,
            cfg.data.workers_per_gpu
        )
        for ds in datasets
    ]
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer, scheduler = create_jittor_optimizer(model, cfg)
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°ç¨³å®šæ€§
    print("æ£€æŸ¥æ¨¡å‹å‚æ•°ç¨³å®šæ€§...")
    try:
        param_stats = {}
        for name, param in model.named_parameters():
            if param.requires_grad:
                param_np = param.detach().numpy()
                param_stats[name] = {
                    'mean': float(np.mean(param_np)),
                    'std': float(np.std(param_np)),
                    'min': float(np.min(param_np)),
                    'max': float(np.max(param_np)),
                    'has_nan': np.any(np.isnan(param_np)),
                    'has_inf': np.any(np.isinf(param_np))
                }
            
    except Exception as e:
        print(f"æ¨¡å‹å‚æ•°æ£€æŸ¥å¤±è´¥: {e}")
    
    # æ¢¯åº¦è£å‰ªé…ç½®ï¼ˆæ¥è‡ª mmdet é…ç½®ï¼‰
    grad_clip_cfg = None
    try:
        if hasattr(cfg, 'optimizer_config') and cfg.optimizer_config is not None:
            grad_clip_cfg = getattr(cfg.optimizer_config, 'grad_clip', None)
    except Exception:
        grad_clip_cfg = None
    
    # å­¦ä¹ ç‡é…ç½®
    step_epochs = []
    if hasattr(cfg, 'lr_config'):
        lr_config = cfg.lr_config
        if hasattr(lr_config, 'type') and lr_config.type == 'MultiStepLR':
            step_epochs = lr_config.get('milestones', [])
            print(f" å­¦ä¹ ç‡è¡°å‡è½®æ¬¡: {step_epochs}")
        elif hasattr(lr_config, 'type') and lr_config.type == 'StepLR':
            step_size = lr_config.get('step', 8)
            if isinstance(step_size, list):
                step_epochs = step_size
            else:
                step_epochs = [step_size]
            print(f" å­¦ä¹ ç‡è¡°å‡è½®æ¬¡: {step_epochs}")
    
    # è®­ç»ƒå¾ªç¯
    # print("ğŸ¯ å¼€å§‹Jittorè®­ç»ƒå¾ªç¯...")
    logger.info("Start Jittor training loop...")
    max_epochs = args.epochs if hasattr(args, 'epochs') else (cfg.runner.max_epochs if hasattr(cfg, 'runner') else 12)
    
    # è®­ç»ƒç»Ÿè®¡
    total_steps = 0
    epoch_losses = []
    
    # JSON æ—¥å¿—å·¥å…·
    def append_json_log(record: dict):
        if not json_log_path:
            return
        try:
            with open(json_log_path, 'a') as jf:
                jf.write(json.dumps(record, ensure_ascii=False) + "\n")
        except Exception:
            pass

    # è®­ç»ƒå¼€å§‹æ—¥å¿—
    print(f"\nå¼€å§‹è®­ç»ƒï¼")
    print(f" æ€»è½®æ¬¡: {max_epochs}")
    print(f" åˆå§‹å­¦ä¹ ç‡: {optimizer.lr:.6f}")
    print(f" æ‰¹æ¬¡å¤§å°: {cfg.data.samples_per_gpu}")
    print(f" å·¥ä½œç›®å½•: {cfg.work_dir}")
    logger.info(f"Training started: epochs={max_epochs}, lr={optimizer.lr:.6f}")
    
    # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
    epoch_records = []
    
    for epoch in range(max_epochs):
        print(f"\nè®­ç»ƒè½®æ¬¡ {epoch + 1}/{max_epochs}")
        print(f" å½“å‰å­¦ä¹ ç‡: {optimizer.lr:.6f}")
        logger.info(f"Epoch [{epoch+1}/{max_epochs}] lr={optimizer.lr:.6f}")
        
        # æ¯ä¸ªepochå¼€å§‹æ—¶æ¸…ç†å†…å­˜
        if epoch > 0:  # ç¬¬ä¸€ä¸ªepochä¸éœ€è¦æ¸…ç†
            clear_jittor_cache()
            gc.collect()
        
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        
        # è½®æ¬¡ç»Ÿè®¡
        epoch_loss = 0.0
        epoch_components = {}
        num_batches = 0
        
        # éå†æ•°æ®åŠ è½½å™¨
        total_batches = len(data_loaders[0])
        print(f"æœ¬è½®æ¬¡æ€»æ‰¹æ¬¡æ•°: {total_batches}")
        
        # æ·»åŠ æ‰¹æ¬¡è®¡æ•°å™¨ï¼Œç¡®ä¿å®é™…å¤„ç†äº†æ‰€æœ‰æ‰¹æ¬¡
        processed_batches = 0
        skipped_batches = 0
        
        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(
            enumerate(data_loaders[0]), 
            total=total_batches,
            desc=f"Epoch {epoch+1}/{max_epochs}",
            leave=True,
            ncols=100
        )
        
        for i, data_batch in pbar:
            
            try:
                # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®æ‰¹æ¬¡çš„æ‰¹æ¬¡å¤§å°
                if i == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ˜¾ç¤º
                    print(f" æ•°æ®æ‰¹æ¬¡è°ƒè¯•ä¿¡æ¯:")
                    print(f"   data_batchç±»å‹: {type(data_batch)}")
                    if 'img' in data_batch:
                        img_data = data_batch['img']
                        print(f"   imgç±»å‹: {type(img_data)}")
                        try:
                            img_var = ensure_jittor_var(img_data, "img_data")
                            print(f"   imgå½¢çŠ¶: {img_var.shape}")
                        except Exception:
                            if isinstance(img_data, (list, tuple)) and len(img_data) > 0:
                                try:
                                    first_img = ensure_jittor_var(img_data[0], "img_data[0]")
                                    print(f"   ç¬¬ä¸€ä¸ªimgå½¢çŠ¶: {first_img.shape}")
                                except Exception:
                                    print(f"   ç¬¬ä¸€ä¸ªimgè½¬æ¢å¤±è´¥")
                            else:
                                print(f"   imgè½¬æ¢å¤±è´¥")
                    print(f"   data_batché”®: {list(data_batch.keys())}")
                
                # ä»…è½¬æ¢å¿…è¦é”®ï¼Œé¿å…å¯¹å¤æ‚å…ƒä¿¡æ¯é€’å½’å¯¼è‡´çš„ __instancecheck__ é€’å½’
                wanted_keys = ['img', 'gt_bboxes', 'gt_labels', 'proposals']
                jt_data = {}
                for key in wanted_keys:
                    if key in data_batch:
                        jt_data[key] = safe_convert_to_jittor(data_batch[key])

                # ä½¿ç”¨æ–°çš„è¾…åŠ©å‡½æ•°ç®€åŒ–ç±»å‹è½¬æ¢
                def to_jt_var(x):
                    """å®‰å…¨åœ°å°†å„ç§æ•°æ®ç±»å‹è½¬æ¢ä¸ºJittor Var"""
                    return ensure_jittor_var(x, "data", None)

                # å¼ºåˆ¶è½¬æ¢æ‰€æœ‰æ•°æ®ä¸ºJittoræ ¼å¼
                if 'img' in jt_data:
                    # å¤„ç†å›¾åƒæ•°æ®ï¼šç¡®ä¿æ˜¯å•ä¸ªå¼ é‡è€Œä¸æ˜¯åˆ—è¡¨
                    try:
                        if isinstance(jt_data['img'], (list, tuple)) and len(jt_data['img']) > 0:
                            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥è½¬æ¢æ•´ä¸ªåˆ—è¡¨ï¼ˆMMDetectionçš„é»˜è®¤è¡Œä¸ºï¼‰
                            jt_data['img'] = to_jt_var(jt_data['img'])
                        else:
                            jt_data['img'] = to_jt_var(jt_data['img'])
                        
                        # ä½¿ç”¨è¾…åŠ©å‡½æ•°ç¡®ä¿å›¾åƒæ•°æ®æ ¼å¼æ­£ç¡®ï¼Œä¸å¼ºåˆ¶æŒ‡å®šå½¢çŠ¶
                        jt_data['img'] = ensure_jittor_var(jt_data['img'], "img")
                        print(f"å›¾åƒæ•°æ®è½¬æ¢å: {jt_data['img'].shape}, ç±»å‹: {type(jt_data['img'])}")
                    except Exception as img_error:
                        print(f"å›¾åƒæ•°æ®è½¬æ¢å¤±è´¥: {img_error}")
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤å€¼
                        jt_data['img'] = jt.zeros((1, 3, 224, 224), dtype='float32')
                        print(f"ä½¿ç”¨é»˜è®¤å›¾åƒå¼ é‡: {jt_data['img'].shape}")
                
                if 'gt_bboxes' in jt_data:
                    # å¤„ç† DataContainer ç±»å‹
                    try:
                        if hasattr(jt_data['gt_bboxes'], 'data'):
                            # å¦‚æœæ˜¯ DataContainerï¼Œæå–å…¶ data å±æ€§
                            jt_data['gt_bboxes'] = jt_data['gt_bboxes'].data
                    except Exception:
                        pass
                    
                    # ä½¿ç”¨æ–°çš„è¾…åŠ©å‡½æ•°ç®€åŒ–è½¬æ¢
                    try:
                        jt_data['gt_bboxes'] = ensure_jittor_var(jt_data['gt_bboxes'], "gt_bboxes")
                        print(f"GT bboxes è½¬æ¢å: {jt_data['gt_bboxes'].shape}, ç±»å‹: {type(jt_data['gt_bboxes'])}")
                    except Exception as bbox_error:
                        print(f"GT bboxes è½¬æ¢å¤±è´¥: {bbox_error}")
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        jt_data['gt_bboxes'] = jt.zeros((1, 4), dtype='float32')
                        print(f"ä½¿ç”¨é»˜è®¤ GT bboxes: {jt_data['gt_bboxes'].shape}")
                
                if 'gt_labels' in jt_data:
                    # å¤„ç† DataContainer ç±»å‹
                    try:
                        if hasattr(jt_data['gt_labels'], 'data'):
                            # å¦‚æœæ˜¯ DataContainerï¼Œæå–å…¶ data å±æ€§
                            jt_data['gt_labels'] = jt_data['gt_labels'].data
                    except Exception:
                        pass
                    
                    # ä½¿ç”¨æ–°çš„è¾…åŠ©å‡½æ•°ç®€åŒ–è½¬æ¢
                    try:
                        jt_data['gt_labels'] = ensure_jittor_var(jt_data['gt_labels'], "gt_labels")
                        print(f"GT labels è½¬æ¢å: {jt_data['gt_labels'].shape}, ç±»å‹: {type(jt_data['gt_labels'])}")
                    except Exception as label_error:
                        print(f"GT labels è½¬æ¢å¤±è´¥: {label_error}")
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        jt_data['gt_labels'] = jt.zeros((1,), dtype='int32')
                        print(f"ä½¿ç”¨é»˜è®¤ GT labels: {jt_data['gt_labels'].shape}")
            
                if 'proposals' in jt_data:
                    # ä½¿ç”¨æ–°çš„è¾…åŠ©å‡½æ•°ç®€åŒ–è½¬æ¢
                    jt_data['proposals'] = ensure_jittor_var(jt_data['proposals'], "proposals")
                
                # æ•°æ®æ ¼å¼éªŒè¯å’Œä¿®å¤
                try:
                    # ç¡®ä¿gt_bboxeså’Œgt_labelsçš„æ ¼å¼æ­£ç¡®
                    if 'gt_bboxes' in jt_data and 'gt_labels' in jt_data:
                        bbox_shape = jt_data['gt_bboxes'].shape
                        label_shape = jt_data['gt_labels'].shape
                        
                        # æ£€æŸ¥gt_bboxesæ ¼å¼ï¼šåº”è¯¥æ˜¯ [N, 4] å…¶ä¸­Næ˜¯è¾¹ç•Œæ¡†æ•°é‡
                        if len(bbox_shape) == 2 and bbox_shape[1] == 4:
                            print(f"gt_bboxesæ ¼å¼æ­£ç¡®: {bbox_shape}")
                        elif len(bbox_shape) == 3 and bbox_shape[2] == 4:
                            # å¦‚æœæ˜¯ [B, N, 4] æ ¼å¼ï¼Œå±•å¹³ä¸º [B*N, 4]
                            jt_data['gt_bboxes'] = jt_data['gt_bboxes'].view(-1, 4)
                            print(f"gt_bboxeså·²å±•å¹³: {jt_data['gt_bboxes'].shape}")
                        else:
                            print(f"gt_bboxesæ ¼å¼å¼‚å¸¸: {bbox_shape}")
                        
                        # æ£€æŸ¥gt_labelsæ ¼å¼ï¼šåº”è¯¥æ˜¯ [N] å…¶ä¸­Næ˜¯æ ‡ç­¾æ•°é‡
                        if len(label_shape) == 1:
                            print(f"gt_labelsæ ¼å¼æ­£ç¡®: {label_shape}")
                        elif len(label_shape) == 2:
                            # å¦‚æœæ˜¯ [B, N] æ ¼å¼ï¼Œå±•å¹³ä¸º [B*N]
                            jt_data['gt_labels'] = jt_data['gt_labels'].view(-1)
                            print(f"gt_labelså·²å±•å¹³: {jt_data['gt_labels'].shape}")
                        else:
                            print(f"gt_labelsæ ¼å¼å¼‚å¸¸: {label_shape}")
                        
                        # ç¡®ä¿è¾¹ç•Œæ¡†å’Œæ ‡ç­¾æ•°é‡ä¸€è‡´
                        bbox_count = jt_data['gt_bboxes'].shape[0]
                        label_count = jt_data['gt_labels'].shape[0]
                        if bbox_count != label_count:
                            print(f"è¾¹ç•Œæ¡†å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…: bboxes={bbox_count}, labels={label_count}")
                            # å–è¾ƒå°çš„æ•°é‡
                            min_count = min(bbox_count, label_count)
                            if bbox_count > min_count:
                                jt_data['gt_bboxes'] = jt_data['gt_bboxes'][:min_count]
                            if label_count > min_count:
                                jt_data['gt_labels'] = jt_data['gt_labels'][:min_count]
                            print(f"å·²è°ƒæ•´æ•°é‡ä¸º: {min_count}")
                except Exception as e:
                    print(f"æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥: {e}")
                

                # # è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ˜¾ç¤ºï¼Œç®€åŒ–è¾“å‡ºï¼‰
                # if i == 0:
                #     print(f"æ•°æ®è°ƒè¯•ä¿¡æ¯:")
                #     for key, value in jt_data.items():
                #         if isinstance(value, jt.Var):
                #             print(f"   {key}: {value.shape}, ç±»å‹: {type(value)}")
                #         elif isinstance(value, (list, tuple)) and len(value) > 0:
                #             print(f"   {key}: list with {len(value)} items")
                #             first_item = ensure_jittor_var(value[0], f"{key}[0]")
                #             print(f"     first item shape: {first_item.shape}, ç±»å‹: {type(first_item)}")
                #         else:
                #             print(f"   {key}: ç±»å‹: {type(value)}")
                
                # å‰å‘ä¼ æ’­
                losses = model(**jt_data)
                
                # è®¡ç®—æ€»æŸå¤±å¹¶è¿›è¡Œç¨³å®šåŒ–
                total_loss = sum(losses.values())
                
                # æ£€æŸ¥æ€»æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                total_loss_val = ensure_jittor_var(total_loss, "total_loss").item()
                
                # ç´¯ç§¯å„é¡¹æŸå¤±
                for key, value in losses.items():
                    if key != 'loss':
                        if key not in epoch_components:
                            epoch_components[key] = 0.0
                        try:
                            epoch_components[key] += ensure_jittor_var(value, f"losses[{key}]").item()
                        except Exception as e:
                            print(f"ç´¯ç§¯æŸå¤±å¤±è´¥ {key}: {e}")
                            epoch_components[key] += 0.0
                else:
                    # å¦‚æœlossesä¸æ˜¯å­—å…¸ï¼Œç¡®ä¿total_lossè¢«æ­£ç¡®å®šä¹‰
                    try:
                        total_loss = ensure_jittor_var(losses, "losses", (1,))
                        # æ£€æŸ¥æ€»æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                        total_loss_val = total_loss.item()
                        if not np.isfinite(total_loss_val) or abs(total_loss_val) > 1000:
                            print(f"WARNING: æ€»æŸå¤± = {total_loss_val} (å¼‚å¸¸å€¼)")
                        logger.warning(f"Abnormal total loss: {total_loss_val}")
                        # å¦‚æœæ€»æŸå¤±æ— æ•ˆï¼Œä½¿ç”¨ä¸€ä¸ªå°çš„é»˜è®¤å€¼
                        total_loss = jt.array(0.001)
                    except Exception as e:
                        print(f"æŸå¤±è½¬æ¢å¤±è´¥: {e}")
                        total_loss = jt.array(0.001)
                
                # æ¸©å’Œåœ°é™åˆ¶æŸå¤±å€¼èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
                try:
                    # å…ˆæ£€æŸ¥æŸå¤±å€¼æ˜¯å¦å¼‚å¸¸
                    loss_val = ensure_jittor_var(total_loss, "total_loss").item()
                    if not np.isfinite(loss_val):
                        print(f"æ£€æµ‹åˆ°éæœ‰é™æŸå¤±å€¼: {loss_val}")
                        # å¦‚æœæŸå¤±å€¼éæœ‰é™ï¼Œä½¿ç”¨ä¸€ä¸ªåŸºäºæ‰¹æ¬¡å¤§å°çš„åˆç†å€¼
                        total_loss = jt.array(0.1 * batch_size)
                        print(f"ä½¿ç”¨åŸºäºæ‰¹æ¬¡å¤§å°çš„æŸå¤±å€¼: {ensure_jittor_var(total_loss, 'total_loss').item()}")
                    elif abs(loss_val) > 10000:  # æé«˜é˜ˆå€¼ï¼Œé¿å…è¿‡åº¦é™åˆ¶
                        print(f"æ£€æµ‹åˆ°è¿‡å¤§æŸå¤±å€¼: {loss_val}")
                        # å¦‚æœæŸå¤±å€¼è¿‡å¤§ï¼Œè¿›è¡Œæ¸©å’Œçš„ç¼©æ”¾
                        scale_factor = 1000.0 / abs(loss_val)
                        total_loss = total_loss * scale_factor
                        print(f"æŸå¤±å€¼å·²ç¼©æ”¾: {loss_val:.2e} -> {ensure_jittor_var(total_loss, 'total_loss').item():.4f}")
                    else:
                        # åªåœ¨æŸå¤±å€¼æ­£å¸¸æ—¶è¿›è¡Œæ¸©å’Œé™åˆ¶
                        total_loss = total_loss.clamp(-1000.0, 1000.0)
                except Exception as e:
                        print(f"æŸå¤±å€¼é™åˆ¶å¤±è´¥: {e}")
                        # å¦‚æœé™åˆ¶å¤±è´¥ï¼Œä½¿ç”¨åŸºäºæ‰¹æ¬¡å¤§å°çš„å€¼
                        total_loss = jt.array(0.1 * batch_size)
                        print(f"ä½¿ç”¨åŸºäºæ‰¹æ¬¡å¤§å°çš„æŸå¤±å€¼: {ensure_jittor_var(total_loss, 'total_loss').item()}")
                
                # åå‘ä¼ æ’­ & æ¢¯åº¦è£å‰ªï¼ˆè‹¥é…ç½®å¯ç”¨ï¼‰
                # print(f"ğŸ”„ å¼€å§‹åå‘ä¼ æ’­...")
                grad_norm_value = None
                if grad_clip_cfg is not None:
                    # åœ¨Jittorä¸­ï¼Œæ¢¯åº¦è£å‰ªé€šå¸¸é€šè¿‡ä¼˜åŒ–å™¨é…ç½®å®ç°ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                    try:
                        max_norm = float(getattr(grad_clip_cfg, 'max_norm', 20))
                        print(f"æ¢¯åº¦è£å‰ªé…ç½®: max_norm={max_norm}")
                    except Exception:
                        pass
                
                # ç®€åŒ–çš„æ¢¯åº¦ç›‘æ§ï¼ˆé¿å…ä½¿ç”¨jt.gradï¼‰
                try:
                    # åœ¨Jittorä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦
                    # æ¢¯åº¦ä¼šåœ¨optimizer.step()ä¸­è‡ªåŠ¨è®¡ç®—
                    pass
                except Exception as e:
                    print(f"æ¢¯åº¦ç›‘æ§å¤±è´¥: {e}")

                
                # æœ€ç»ˆæ£€æŸ¥ total_loss æ˜¯å¦è¢«æ­£ç¡®å®šä¹‰
                if total_loss is None:
                    print(f"total_loss ä»ç„¶ä¸º Noneï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    total_loss = jt.array(0.001)
                
                # æ›´æ–°å‚æ•°
                try:
                    # åœ¨Jittorä¸­ï¼Œä½¿ç”¨optimizer.step(loss)æ¥è‡ªåŠ¨å¤„ç†æ¢¯åº¦è®¡ç®—å’Œæ›´æ–°
                    # ä½¿ç”¨è¾…åŠ©å‡½æ•°ç¡®ä¿total_lossæ˜¯å•ä¸ªJittorå¼ é‡
                    total_loss = ensure_jittor_var(total_loss, "total_loss", (1,))
                    
                    print(f"ä¼˜åŒ–å™¨æ›´æ–°å‰ï¼Œtotal_lossç±»å‹: {type(total_loss)}, shape: {ensure_jittor_var(total_loss, 'total_loss').shape}")

                    # åœ¨Jittorä¸­ï¼Œæ¨èä½¿ç”¨ optimizer.step(loss) æ¥è‡ªåŠ¨å¤„ç†
                    optimizer.step(total_loss)
                    
                    processed_batches += 1  # æˆåŠŸå¤„ç†çš„æ‰¹æ¬¡

                except Exception as e:
                    print(f"ä¼˜åŒ–å™¨æ›´æ–°å¤±è´¥: {e}")
                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ¸…ç†å†…å­˜å¹¶ç»§ç»­
                    try:
                        clear_jittor_cache()
                        jt.sync_all()
                    except:
                        pass
                    logger.error(f"Optimizer step failed: {e}")
                    # å¦‚æœä¼˜åŒ–å™¨æ›´æ–°å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ‰¹æ¬¡
                    skipped_batches += 1
                    continue
                
                # å†…å­˜ç®¡ç†ä¼˜åŒ–ï¼šæ¸…ç†ä¸­é—´å˜é‡å’Œæ¢¯åº¦
                try:
                    # åœ¨Jittorä¸­ï¼Œä¸éœ€è¦æ‰‹åŠ¨æ¸…ç†æ¢¯åº¦ï¼Œoptimizer.step()ä¼šè‡ªåŠ¨å¤„ç†
                    # æ¸…ç†Jittorç¼“å­˜
                    clear_jittor_cache()
                    
                    # æ¸…ç†ä¸­é—´å˜é‡å¼•ç”¨
                    del total_loss
                    if 'losses' in locals():
                        del losses
                    if 'jt_data' in locals():
                        del jt_data
                    
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                    
                        
                except Exception as e:
                    print(f"å†…å­˜æ¸…ç†å¤±è´¥: {e}")
                
                # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if scheduler is not None:
                    try:
                        # æ£€æŸ¥å½“å‰å­¦ä¹ ç‡æ˜¯å¦è¿‡ä½
                        current_lr = optimizer.lr
                        if current_lr < 1e-6:  # å¦‚æœå­¦ä¹ ç‡è¿‡ä½ï¼Œé‡ç½®ä¸ºåˆå§‹å€¼
                            print(f"å­¦ä¹ ç‡è¿‡ä½ ({current_lr:.2e})ï¼Œé‡ç½®ä¸ºåˆå§‹å€¼")
                            optimizer.lr = 0.005  # é‡ç½®ä¸ºåˆå§‹å­¦ä¹ ç‡
                        
                        scheduler.step()
                        
                        # å®‰å…¨åœ°è·å–å½“å‰å­¦ä¹ ç‡
                        if hasattr(optimizer, 'param_groups') and len(optimizer.param_groups) > 0:
                            current_lr = optimizer.param_groups[0].get('lr', 0.0)
                            if i % 200 == 0:  # æ¯200æ­¥æ‰“å°ä¸€æ¬¡å­¦ä¹ ç‡
                                print(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")
                    except Exception as e:
                        print(f"å­¦ä¹ ç‡è°ƒåº¦å™¨æ›´æ–°å¤±è´¥: {e}")
                        # å¦‚æœè°ƒåº¦å™¨æ›´æ–°å¤±è´¥ï¼Œå°è¯•é‡ç½®
                        try:
                            if hasattr(scheduler, 'reset'):
                                scheduler.reset()
                                print("å­¦ä¹ ç‡è°ƒåº¦å™¨å·²é‡ç½®")
                        except Exception as e2:
                            print(f"âš ï¸  å­¦ä¹ ç‡è°ƒåº¦å™¨é‡ç½®ä¹Ÿå¤±è´¥: {e2}")
                
                # ç´¯ç§¯æŸå¤±
                try:
                    if 'total_loss' in locals() and total_loss is not None:
                        epoch_loss += ensure_jittor_var(total_loss, "total_loss").item()
                    else:
                        print(f"âš ï¸  total_loss æœªå®šä¹‰ï¼Œè·³è¿‡ç´¯ç§¯")
                        epoch_loss += 0.0
                except Exception as e:
                    print(f"âš ï¸  ç´¯ç§¯æ€»æŸå¤±å¤±è´¥: {e}")
                    epoch_loss += 0.0
                
                num_batches += 1
                total_steps += 1

                # å‘¨æœŸæ€§å›æ”¶æ˜¾å­˜ï¼Œç¼“è§£ OOMï¼ˆJittor æ¨èï¼‰
                if (i + 1) % 50 == 0:  # æ›´é¢‘ç¹çš„å†…å­˜æ¸…ç†
                    try:
                        jt.gc()
                        clear_jittor_cache()
                        gc.collect()
                    except Exception:
                        pass
            
                
                # æ›´æ–°tqdmè¿›åº¦æ¡æ˜¾ç¤ºæŸå¤±ä¿¡æ¯
                if isinstance(losses, dict):
                    # åªæ˜¾ç¤ºä¸»è¦çš„æŸå¤±å€¼ï¼Œé¿å…ä¿¡æ¯è¿‡å¤š
                    main_losses = {}
                    for k, v in losses.items():
                        if k in ['loss', 'rpn_cls_loss', 'rpn_bbox_loss', 'rcnn_cls_loss', 'rcnn_bbox_loss']:
                            try:
                                main_losses[k] = f"{ensure_jittor_var(v, f'losses[{k}]').item():.4f}"
                            except:
                                main_losses[k] = "0.0000"
                    
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    pbar.set_postfix({
                        'Loss': f"{ensure_jittor_var(total_loss, 'total_loss').item():.4f}",
                        'RPN': f"{main_losses.get('rpn_cls_loss', '0.0000')}",
                        'RCNN': f"{main_losses.get('rcnn_cls_loss', '0.0000')}"
                    })
                else:
                    pbar.set_postfix({'Loss': f"{ensure_jittor_var(total_loss, 'total_loss').item():.4f}"})
                
                # æ¯100æ­¥è®°å½•åˆ°loggerå’ŒJSONæ—¥å¿—
                if i % 100 == 0:
                    if isinstance(losses, dict):
                        loss_str = ', '.join([f'{k}: {ensure_jittor_var(v, f"losses[{k}]").item():.4f}' for k, v in losses.items()])
                    else:
                        loss_str = f'{ensure_jittor_var(total_loss, "total_loss").item():.4f}'
                    
                    # è®°å½•åˆ°logger
                    logger.info(f"Step {i+1}: {loss_str}")
                    
                    # JSON è¡Œæ—¥å¿—ï¼ˆä¸MMDeté£æ ¼æ¥è¿‘ï¼‰
                    record = {
                        'mode': 'train',
                        'epoch': epoch + 1,
                        'iter': i + 1,
                        'lr': float(optimizer.lr),
                        'total_batches': total_batches,
                        'grad_norm': float(grad_norm) if 'grad_norm' in locals() else 0.0,
                    }
                    if grad_norm_value is not None:
                        record['grad_norm'] = float(grad_norm_value)
                    if isinstance(losses, dict):
                        for k, v in losses.items():
                            try:
                                record[k] = float(ensure_jittor_var(v, f"losses[{k}]").item())
                            except Exception:
                                pass
                        record['loss'] = float(ensure_jittor_var(total_loss, "total_loss").item())
                    else:
                        record['loss'] = float(ensure_jittor_var(total_loss, "total_loss").item())
                    append_json_log(record)
                    
            except Exception as e:
                # åªåœ¨ç¬¬ä¸€ä¸ªé”™è¯¯æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œåç»­é”™è¯¯é™é»˜å¤„ç†
                if num_batches == 0:
                    print(f"âŒ æ‰¹æ¬¡ {i+1} å¤„ç†å¤±è´¥: {e}")
                    import traceback as _tb
                    _tb.print_exc()
                    print(f"   æ•°æ®ç±»å‹: {type(data_batch)}")
                    if isinstance(data_batch, dict):
                        for key, value in data_batch.items():
                            print(f"   {key}: {type(value)}")
                skipped_batches += 1
                continue
        
        # è®¡ç®—å¹³å‡æŸå¤±
        try:
            avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0
            # æ£€æŸ¥å¹³å‡æŸå¤±æ˜¯å¦æœ‰æ•ˆ
            if not np.isfinite(avg_loss):
                print(f"WARNING: å¹³å‡æŸå¤± = {avg_loss} (éæœ‰é™å€¼)ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                avg_loss = 0.001
        except Exception as e:
            print(f"è®¡ç®—å¹³å‡æŸå¤±å¤±è´¥: {e}")
            avg_loss = 0.001
        
        try:
            avg_components = {key: value / num_batches if num_batches > 0 else 0.0 
                             for key, value in epoch_components.items()}
            # æ£€æŸ¥ç»„ä»¶æŸå¤±æ˜¯å¦æœ‰æ•ˆ
            for key, value in avg_components.items():
                if not np.isfinite(value):
                    print(f" WARNING: {key} = {value} (éæœ‰é™å€¼)ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    avg_components[key] = 0.0
        except Exception as e:
            print(f"è®¡ç®—å¹³å‡ç»„ä»¶æŸå¤±å¤±è´¥: {e}")
            avg_components = {}
        
        epoch_losses.append(avg_loss)
        
        # å…³é—­tqdmè¿›åº¦æ¡
        pbar.close()
        
        print(f"\nè½®æ¬¡ {epoch + 1} ç»Ÿè®¡:")
        print(f"   - å¹³å‡æ€»æŸå¤±: {avg_loss:.4f}")
        if avg_components:
            for key, value in avg_components.items():
                print(f"   - {key}: {value:.4f}")
        print(f"   - æ€»æ­¥æ•°: {total_steps}")
        print(f"   - æˆåŠŸå¤„ç†æ‰¹æ¬¡: {processed_batches}")
        print(f"   - è·³è¿‡æ‰¹æ¬¡: {skipped_batches}")
        print(f"   - å®é™…å¤„ç†æ‰¹æ¬¡: {num_batches}/{total_batches} ({num_batches/total_batches*100:.1f}%)")
        
        # è®°å½•åˆ°logger
        logger.info(
            f"Epoch {epoch+1} avg_loss={avg_loss:.4f}, steps={total_steps}, processed_batches={processed_batches}, skipped_batches={skipped_batches}"
        )
        
        # è®°å½•åˆ°JSONæ—¥å¿—
        epoch_record = {
            'mode': 'epoch_summary',
            'epoch': epoch + 1,
            'avg_loss': float(avg_loss),
            'total_steps': total_steps,
            'processed_batches': processed_batches,
            'skipped_batches': skipped_batches,
            'num_batches': num_batches,
            'total_batches': total_batches,
            'lr': float(optimizer.lr)
        }
        if avg_components:
            for key, value in avg_components.items():
                epoch_record[f'avg_{key}'] = float(value)
        append_json_log(epoch_record)
        
        # ä¿å­˜epochè®°å½•
        epoch_records.append(epoch_record)
        
        # å­¦ä¹ ç‡è¡°å‡
        if step_epochs and (epoch + 1) in step_epochs:
            old_lr = optimizer.lr
            optimizer.lr *= 0.1
            print(f"å­¦ä¹ ç‡è¡°å‡: {old_lr:.6f} -> {optimizer.lr:.6f}")
        
        # éªŒè¯
        if validate and len(datasets) > 1:
            print(f"è¿›è¡ŒéªŒè¯...")
            model.eval()
        
        # æ˜¾ç¤ºå½“å‰epochå®ŒæˆçŠ¶æ€
        print(f"Epoch {epoch+1} å®Œæˆæ—¶é—´: {datetime.datetime.now().strftime('%H:%M:%S')}")
        
        # æ¯ä¸ªepochç»“æŸæ—¶æ¸…ç†å†…å­˜
        clear_jittor_cache()
        gc.collect()
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if hasattr(cfg, 'checkpoint_config') and cfg.checkpoint_config.interval > 0:
            if (epoch + 1) % cfg.checkpoint_config.interval == 0:
                # ç»Ÿä¸€ä½¿ç”¨ .pth æ‰©å±•åï¼Œä¾¿äºä¸ PyTorch æµç¨‹å¯¹é½
                checkpoint_path = osp.join(cfg.work_dir, f'epoch_{epoch+1}.pth')
                # å®é™…ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆJittor Var -> numpyï¼‰ï¼Œå¹¶åŒ…å«åŸºæœ¬å…ƒä¿¡æ¯
                try:
                    print(f"å¼€å§‹ä¿å­˜æ£€æŸ¥ç‚¹...")
                    
                    try:
                        jt.sync_all(True)
                        print(f"CUDA åŒæ­¥å®Œæˆ")
                    except Exception as e:
                        print(f" CUDA åŒæ­¥è­¦å‘Š: {e}")
                    
                    # ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
                    import time
                    time.sleep(1)
                    
                    # è·å–æ¨¡å‹çŠ¶æ€
                    print(f"è·å–æ¨¡å‹çŠ¶æ€...")
                    state = {}
                    try:
                        model_state = model.state_dict()
                        print(f"æ¨¡å‹çŠ¶æ€è·å–æˆåŠŸï¼ŒåŒ…å« {len(model_state)} ä¸ªå‚æ•°")
                    except Exception as e:
                        print(f"æ¨¡å‹çŠ¶æ€è·å–å¤±è´¥: {e}")
                        model_state = {}
                    
                    # è½¬æ¢å‚æ•°ä¸º numpy
                    print(f"è½¬æ¢å‚æ•°æ ¼å¼...")
                    for key, val in model_state.items():
                        try:
                            if hasattr(val, 'numpy'):
                                state[key] = val.numpy()
                            elif hasattr(val, 'detach') and hasattr(val, 'cpu'):
                                # å¤„ç†å¯èƒ½çš„ torch.Tensor
                                state[key] = val.detach().cpu().numpy()
                            else:
                                state[key] = val
                        except Exception as e:
                            print(f"âš ï¸  å‚æ•° {key} è½¬æ¢å¤±è´¥: {e}")
                            state[key] = val
                    
                    # å‡†å¤‡å…ƒä¿¡æ¯
                    meta = {
                        'epoch': epoch + 1,
                        'classes': getattr(model, 'CLASSES', None),
                        'config': getattr(cfg, 'pretty_text', None),
                        'timestamp': datetime.datetime.now().isoformat(),
                        'avg_loss': float(avg_loss),
                        'num_batches': num_batches
                    }
                    
                    # åˆ›å»ºç›®å½•å¹¶ä¿å­˜
                    print(f"åˆ›å»ºä¿å­˜ç›®å½•...")
                    mmcv.mkdir_or_exist(osp.dirname(osp.abspath(checkpoint_path)))
                    
                    print(f"å†™å…¥æ£€æŸ¥ç‚¹æ–‡ä»¶...")
                    with open(checkpoint_path, 'wb') as f:
                        pickle.dump({'state_dict': state, 'meta': meta}, f)
                    
                    print(f"æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {checkpoint_path}")
                    logger.info(f"Checkpoint saved to {checkpoint_path}")
                    
                except Exception as e:
                    print(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                    logger.error(f"Failed to save checkpoint: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    # å°è¯•ä¿å­˜ä¸€ä¸ªç®€åŒ–çš„æ£€æŸ¥ç‚¹
                    try:
                        print(f" å°è¯•ä¿å­˜ç®€åŒ–æ£€æŸ¥ç‚¹...")
                        simple_checkpoint_path = osp.join(cfg.work_dir, f'epoch_{epoch+1}_simple.pth')
                        simple_state = {'epoch': epoch + 1, 'error': str(e)}
                        with open(simple_checkpoint_path, 'wb') as f:
                            pickle.dump(simple_state, f)
                        print(f"ç®€åŒ–æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {simple_checkpoint_path}")
                    except Exception as e2:
                        print(f"ç®€åŒ–æ£€æŸ¥ç‚¹ä¹Ÿä¿å­˜å¤±è´¥: {e2}")
        
        print(f"è½®æ¬¡ {epoch + 1} å®Œæˆ")
    
    # è®­ç»ƒå®Œæˆæ€»ç»“
    print(f"\n Jittorè®­ç»ƒå®Œæˆ!")
    print(f"è®­ç»ƒç»Ÿè®¡:")
    print(f"   - æ€»è½®æ¬¡: {max_epochs}")
    print(f"   - æœ€ç»ˆå¹³å‡æŸå¤±: {np.mean(epoch_losses):.4f}")
    print(f"   - æ€»æ­¥æ•°: {total_steps}")
    print(f"   - æ€»æˆåŠŸæ‰¹æ¬¡: {sum([epoch_record.get('processed_batches', 0) for epoch_record in epoch_records if 'processed_batches' in epoch_record])}")
    print(f"   - æ€»è·³è¿‡æ‰¹æ¬¡: {sum([epoch_record.get('skipped_batches', 0) for epoch_record in epoch_records if 'skipped_batches' in epoch_record])}")
    
    # è®°å½•åˆ°logger
    logger.info(f"Training completed: epochs={max_epochs}, final_avg_loss={np.mean(epoch_losses):.4f}, total_steps={total_steps}")
    
    # è®°å½•åˆ°JSONæ—¥å¿—
    final_record = {
        'mode': 'training_complete',
        'total_epochs': max_epochs,
        'final_avg_loss': float(np.mean(epoch_losses)),
        'total_steps': total_steps,
        'timestamp': datetime.datetime.now().isoformat()
    }
    append_json_log(final_record)